{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyPm0IvkFVpDNmH4SiRlRkI5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"9d0a84780ec844a2b56e62595e4ea546":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_196744c857fc452a9dbf1d70e9fd01e7","IPY_MODEL_893fc9abbfa64b5fbaa2ab279a14a723","IPY_MODEL_da6bf22606f24e298ea752830c90cb8d"],"layout":"IPY_MODEL_2a318e6b4bd24de0921ff1b17fdb798a"}},"196744c857fc452a9dbf1d70e9fd01e7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b096709ab6344ae81cf30b73bd2b2a6","placeholder":"​","style":"IPY_MODEL_862bcaf6aa3d4b999b631b6a9cc41d0a","value":"Loading checkpoint shards: 100%"}},"893fc9abbfa64b5fbaa2ab279a14a723":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fdbc9c7b96684628b4e18ca40da4c9cc","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0978507310bf4775aea9c9e4e830d2f5","value":2}},"da6bf22606f24e298ea752830c90cb8d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_be00f38974c142208eeadaa66cfecdfa","placeholder":"​","style":"IPY_MODEL_9cca94219596472c9ed6f8992495d31f","value":" 2/2 [00:26&lt;00:00, 11.92s/it]"}},"2a318e6b4bd24de0921ff1b17fdb798a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b096709ab6344ae81cf30b73bd2b2a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"862bcaf6aa3d4b999b631b6a9cc41d0a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fdbc9c7b96684628b4e18ca40da4c9cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0978507310bf4775aea9c9e4e830d2f5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"be00f38974c142208eeadaa66cfecdfa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9cca94219596472c9ed6f8992495d31f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0a01999060184b2cb2f7fa49fc7ea2ca":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2d68e76fd3de44fdbdf70ce4f4cf56a5","IPY_MODEL_f33ae5b3e1bd48fc8670b85f80ab3c2e","IPY_MODEL_e70394190082444e9015ccb2d24c042d"],"layout":"IPY_MODEL_5703d63fed2a4a89baec743a8e62ee88"}},"2d68e76fd3de44fdbdf70ce4f4cf56a5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_436f4603c40b4621a123d96cf3065248","placeholder":"​","style":"IPY_MODEL_396a9b55609548168490e4d74c5cc52c","value":"Loading checkpoint shards: 100%"}},"f33ae5b3e1bd48fc8670b85f80ab3c2e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c6b583a61ff41f593abdd566ef82bcc","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_823d5f70ef1f40da85d16ba7852d6474","value":2}},"e70394190082444e9015ccb2d24c042d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_417b3d9c87434dd0a6391bc19f0738f6","placeholder":"​","style":"IPY_MODEL_0fb9b775ba6c4e7b8fed957c73de8a0d","value":" 2/2 [00:26&lt;00:00, 11.93s/it]"}},"5703d63fed2a4a89baec743a8e62ee88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"436f4603c40b4621a123d96cf3065248":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"396a9b55609548168490e4d74c5cc52c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8c6b583a61ff41f593abdd566ef82bcc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"823d5f70ef1f40da85d16ba7852d6474":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"417b3d9c87434dd0a6391bc19f0738f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0fb9b775ba6c4e7b8fed957c73de8a0d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"79c30191b7214cf598c2bb1a883a8cdb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_555e650fd5544f838921fc993186b2f6","IPY_MODEL_a67cd84efcec4d4896f41955e272c9ab","IPY_MODEL_600d532ac4a144a7990d9fa22e6dd4b8"],"layout":"IPY_MODEL_e8c5340032c748f58e3b452713f3cfd3"}},"555e650fd5544f838921fc993186b2f6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c96412a7509f4b7ea1d232feafe4f414","placeholder":"​","style":"IPY_MODEL_38f172da07c541bc9901d20ab0b8d50b","value":"Loading checkpoint shards: 100%"}},"a67cd84efcec4d4896f41955e272c9ab":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c54dba6d08624c89acc65026ec65fa4f","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_941532563bf84f09935623eb9e0772ac","value":2}},"600d532ac4a144a7990d9fa22e6dd4b8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b0c79001d814488c807dede50d9ec315","placeholder":"​","style":"IPY_MODEL_3b2a8691fb3043e59e4c07def420c428","value":" 2/2 [00:05&lt;00:00,  2.99s/it]"}},"e8c5340032c748f58e3b452713f3cfd3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c96412a7509f4b7ea1d232feafe4f414":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38f172da07c541bc9901d20ab0b8d50b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c54dba6d08624c89acc65026ec65fa4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"941532563bf84f09935623eb9e0772ac":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b0c79001d814488c807dede50d9ec315":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b2a8691fb3043e59e4c07def420c428":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":190,"referenced_widgets":["9d0a84780ec844a2b56e62595e4ea546","196744c857fc452a9dbf1d70e9fd01e7","893fc9abbfa64b5fbaa2ab279a14a723","da6bf22606f24e298ea752830c90cb8d","2a318e6b4bd24de0921ff1b17fdb798a","8b096709ab6344ae81cf30b73bd2b2a6","862bcaf6aa3d4b999b631b6a9cc41d0a","fdbc9c7b96684628b4e18ca40da4c9cc","0978507310bf4775aea9c9e4e830d2f5","be00f38974c142208eeadaa66cfecdfa","9cca94219596472c9ed6f8992495d31f"]},"id":"VPF1UFtrEAyE","executionInfo":{"status":"ok","timestamp":1730541672525,"user_tz":-540,"elapsed":43138,"user":{"displayName":"Coffee Kiosk","userId":"02539563182731550933"}},"outputId":"d65a0d64-7c6d-4f03-a787-22ae7688bf90"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d0a84780ec844a2b56e62595e4ea546"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Coffee Kiosk initialized and ready to take orders.\n"]}],"source":["# Import necessary libraries\n","from transformers import AutoModelForCausalLM, AutoTokenizer  # Assuming Bllossom's llama model uses the Hugging Face transformers library\n","import torch\n","import json\n","\n","# Define our coffee kiosk class to handle orders, responses, and JSON generation\n","class CoffeeKiosk:\n","    def __init__(self):\n","        # Model name for Bllossom's Llama\n","        model_name = \"Bllossom/llama-3.2-Korean-Bllossom-3B\"\n","\n","        # Initialize the tokenizer and model\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n","\n","        # Available menu items\n","        self.menu_items = [\n","            \"아메리카노\", \"라떼\", \"카푸치노\", \"카페모카\", \"바닐라라떼\",\n","            \"에스프레소\", \"카라멜마끼아또\", \"허브티\", \"홍차\", \"초콜릿라떼\",\n","            \"레몬에이드\", \"복숭아아이스티\", \"딸기스무디\", \"망고스무디\",\n","            \"키위주스\", \"토마토주스\"\n","        ]\n","\n","        # Initialize order history and defaults\n","        self.order_history = []\n","        self.default_size = \"미디움\"\n","        self.default_temperature = \"핫\"\n","\n","    # Function to reset order history for a new session\n","    def reset_order(self):\n","        self.order_history = []\n","\n","    # Function to check if a drink is available in the menu\n","    def is_menu_item(self, drink_name):\n","        return drink_name in self.menu_items\n","\n","# Initialize CoffeeKiosk instance\n","kiosk = CoffeeKiosk()\n","\n","print(\"Coffee Kiosk initialized and ready to take orders.\")\n","\n"]},{"cell_type":"code","source":["class CoffeeKiosk:\n","    def __init__(self):\n","        # Initialize menu, order history, and defaults\n","        self.menu_items = [\n","            \"아메리카노\", \"라떼\", \"카푸치노\", \"카페모카\", \"바닐라라떼\",\n","            \"에스프레소\", \"카라멜마끼아또\", \"허브티\", \"홍차\", \"초콜릿라떼\",\n","            \"레몬에이드\", \"복숭아아이스티\", \"딸기스무디\", \"망고스무디\",\n","            \"키위주스\", \"토마토주스\"\n","        ]\n","        self.order_history = []\n","        self.default_size = \"미디움\"\n","        self.default_temperature = \"핫\"\n","        self.shorthand_mapping = {\n","            \"아아\": \"아이스 아메리카노\",\n","            \"뜨아\": \"핫 아메리카노\"\n","        }\n","\n","    def reset_order(self):\n","        \"\"\"Reset the order history for a new customer session.\"\"\"\n","        self.order_history = []\n","\n","    def is_menu_item(self, drink_name):\n","        \"\"\"Check if a drink is on the menu, including shorthand names.\"\"\"\n","        return drink_name in self.menu_items or drink_name in self.shorthand_mapping\n","\n","    def suggest_alternative(self, drink_name):\n","        \"\"\"Suggest a similar item if requested item is unavailable.\"\"\"\n","        if drink_name == \"초코라떼\":\n","            return \"초콜릿라떼\"\n","        return None\n","\n","    def add_order_item(self, drink, quantity=1, temperature=None, size=None, add_ons=None, extra_shots=0):\n","        \"\"\"Add a new item to the order.\"\"\"\n","        # Map shorthand to full drink name if necessary\n","        drink = self.shorthand_mapping.get(drink, drink)\n","\n","        # Default values if not specified\n","        size = size or self.default_size\n","        temperature = temperature or self.default_temperature\n","        add_ons = add_ons or []\n","\n","        item = {\n","            \"drink\": drink,\n","            \"quantity\": quantity,\n","            \"temperature\": temperature,\n","            \"size\": size,\n","            \"add_ons\": add_ons,\n","            \"extra_shots\": extra_shots\n","        }\n","\n","        self.order_history.append(item)\n","\n","        latest_item_response = f\"{drink} {quantity}잔 주문되었습니다.\"\n","        full_summary = self.get_order_summary()\n","        response = f\"{latest_item_response} {full_summary}\"\n","\n","        # JSON output only for the latest item\n","        json_output = {\n","            \"action\": \"create_order\",\n","            \"order_items\": [new_item]\n","        }\n","        return response, json_output\n","\n","    def modify_order(self, old_drink, new_drink, temperature=None, size=None, add_ons=None, extra_shots=0):\n","        \"\"\"Modify an existing order item.\"\"\"\n","        # Locate the item in order history\n","        for item in self.order_history:\n","            if item[\"drink\"] == old_drink:\n","                # Update details\n","                item.update({\n","                    \"drink\": new_drink,\n","                    \"temperature\": temperature or item[\"temperature\"],\n","                    \"size\": size or item[\"size\"],\n","                    \"add_ons\": add_ons or item[\"add_ons\"],\n","                    \"extra_shots\": extra_shots or item[\"extra_shots\"]\n","                })\n","\n","                response = f\"주문이 {old_drink}에서 {new_drink}로 변경되었습니다. {self.get_order_summary()}\"\n","                json_output = {\n","                    \"action\": \"modify_order\",\n","                    \"old_drink\": old_drink,\n","                    \"new_drink\": new_drink,\n","                    \"temperature\": item[\"temperature\"],\n","                    \"size\": item[\"size\"],\n","                    \"quantity\": item[\"quantity\"],\n","                    \"add_ons\": item[\"add_ons\"],\n","                    \"extra_shots\": item[\"extra_shots\"]\n","                }\n","                return response, json_output\n","\n","        # If not found\n","        return f\"{old_drink}는 주문 내역에 없습니다.\", None\n","\n","    def get_order_summary(self):\n","      \"\"\"Generate a comprehensive summary of the current order history.\"\"\"\n","      if not self.order_history:\n","        return \"현재 주문된 항목이 없습니다.\"\n","\n","      summary = \"주문하신 내용은 다음과 같습니다:\"\n","      for item in self.order_history:\n","            drink_summary = (\n","                f\"- {item['drink']} {item['quantity']}잔 \"\n","                f\"({item['temperature']}, {item['size']}\"\n","            )\n","            if item['add_ons']:\n","                drink_summary += f\", 추가 옵션: {', '.join(item['add_ons'])}\"\n","            if item['extra_shots']:\n","                drink_summary += f\", 샷 추가: {item['extra_shots']}회\"\n","            drink_summary += \")\"\n","            summary += f\"\\n{drink_summary}\"\n","      return summary\n","\n","    def summarize_order_history(self):\n","        \"\"\"Provide a summary of the entire order history without adding new JSON output.\"\"\"\n","        if not self.order_history:\n","           return \"현재 주문된 항목이 없습니다.\", None\n","\n","           response = f\"지금까지의 주문내역입니다:\\n{self.get_order_summary()}\"\n","           return response, None\n","\n","    def process_input(self, input_text):\n","        \"\"\"Process customer input and route to the correct functionality.\"\"\"\n","        # Check for order completion\n","        if \"주문 완료할게\" in input_text:\n","            response = \"주문이 완료되었습니다. 결제는 카드리더기를 사용해주세요. 감사합니다.\"\n","            json_output = {\n","                \"action\": \"complete_order\",\n","                \"order_items\": self.order_history\n","            }\n","            self.reset_order()\n","            return response, json_output\n","\n","        if \"지금까지 뭘 주문했지\" in input_text:\n","            response = self.get_order_summary()\n","            json_output = None  # No JSON output for history summary\n","            return response, json_output\n","\n","        # Handle new order or modification based on input\n","        drink = None\n","        for item in self.menu_items + list(self.shorthand_mapping.keys()):\n","            if item in input_text:\n","                drink = self.shorthand_mapping.get(item, item)\n","                break\n","\n","        # If drink is not found, suggest alternatives\n","        if not drink:\n","            alternative = self.suggest_alternative(input_text)\n","            if alternative:\n","                response = f\"죄송합니다, {input_text}는 메뉴에 없습니다. 대신 {alternative}를 추천드립니다.\"\n","                json_output = {\n","                    \"action\": \"recommend_closest_item\",\n","                    \"requested_item\": input_text,\n","                    \"recommended_item\": alternative\n","                }\n","            else:\n","                response = f\"죄송합니다, {input_text}는 메뉴에 없습니다.\"\n","                json_output = None\n","            return response, json_output\n","\n","        # Process adding the new item to the order\n","        parsed_order = self.parse_order_input(input_text)\n","        if parsed_order:\n","            response, json_output = self.add_order_item(\n","                drink=parsed_order[\"drink\"],\n","                quantity=parsed_order.get(\"quantity\", 1),\n","                temperature=parsed_order.get(\"temperature\"),\n","                size=parsed_order.get(\"size\"),\n","                add_ons=parsed_order.get(\"add_ons\", []),\n","                extra_shots=parsed_order.get(\"extra_shots\", 0)\n","            )\n","            return response, json_output\n","\n","\n","    def parse_order_input(self, input_text):\n","        \"\"\"Parse input text to determine drink details.\"\"\"\n","        # Example parsing logic\n","        drink = None\n","        for item in self.menu_items:\n","            if item in input_text:\n","                drink = item\n","                break\n","\n","        if not drink:\n","            return None\n","\n","        quantity = 1\n","        if \"두잔\" in input_text:\n","            quantity = 2\n","        elif \"세잔\" in input_text:\n","            quantity = 3\n","\n","        temperature = \"아이스\" if \"아이스\" in input_text else None\n","        size = \"라지\" if \"라지\" in input_text else None\n","\n","        return {\n","            \"drink\": drink,\n","            \"quantity\": quantity,\n","            \"temperature\": temperature,\n","            \"size\": size\n","        }\n"],"metadata":{"id":"tm66uW6LUm3H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from llama import LlamaModel  # Adjust import based on your setup\n","import json\n","\n","# Initialize the Llama3 model\n","model = LlamaModel(model_name=\"llama3-3b\", temperature=0.7, max_length=1024)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"id":"ycFAhX0fgbqF","executionInfo":{"status":"error","timestamp":1730544784217,"user_tz":-540,"elapsed":8,"user":{"displayName":"Coffee Kiosk","userId":"02539563182731550933"}},"outputId":"b588769a-b21a-4ae1-f709-413c1cd2fb97"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'llama'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-53198c6c9487>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mllama\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLlamaModel\u001b[0m  \u001b[0;31m# Adjust import based on your setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Initialize the Llama3 model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLlamaModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"llama3-3b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["# Import necessary libraries\n","import json\n","\n","# Define our coffee kiosk class to handle orders, responses, and JSON generation\n","class CoffeeKiosk:\n","    def __init__(self):\n","        # Initialize menu, order history, and defaults\n","        self.menu_items = [\n","            \"아메리카노\", \"라떼\", \"카푸치노\", \"카페모카\", \"바닐라라떼\",\n","            \"에스프레소\", \"카라멜마끼아또\", \"허브티\", \"홍차\", \"초콜릿라떼\",\n","            \"레몬에이드\", \"복숭아아이스티\", \"딸기스무디\", \"망고스무디\",\n","            \"키위주스\", \"토마토주스\"\n","        ]\n","        self.order_history = []\n","        self.default_size = \"미디움\"\n","        self.default_temperature = \"핫\"\n","        self.shorthand_mapping = {\n","            \"아아\": \"아이스 아메리카노\",\n","            \"뜨아\": \"핫 아메리카노\"\n","        }\n","\n","    def reset_order(self):\n","        \"\"\"Reset the order history for a new customer session.\"\"\"\n","        self.order_history = []\n","\n","    def is_menu_item(self, drink_name):\n","        \"\"\"Check if a drink is on the menu, including shorthand names.\"\"\"\n","        return drink_name in self.menu_items or drink_name in self.shorthand_mapping\n","\n","    def suggest_alternative(self, drink_name):\n","        \"\"\"Suggest a similar item if requested item is unavailable.\"\"\"\n","        if drink_name == \"초코라떼\":\n","            return \"초콜릿라떼\"\n","        return None\n","\n","    def add_order_item(self, drink, quantity=1, temperature=None, size=None, add_ons=None, extra_shots=0):\n","        \"\"\"Add a new item to the order.\"\"\"\n","        # Map shorthand to full drink name if necessary\n","        drink = self.shorthand_mapping.get(drink, drink)\n","\n","        # Default values if not specified\n","        size = size or self.default_size\n","        temperature = temperature or self.default_temperature\n","        add_ons = add_ons or []\n","\n","        item = {\n","            \"drink\": drink,\n","            \"quantity\": quantity,\n","            \"temperature\": temperature,\n","            \"size\": size,\n","            \"add_ons\": add_ons,\n","            \"extra_shots\": extra_shots\n","        }\n","\n","        self.order_history.append(item)\n","\n","        latest_item_response = f\"{drink} {quantity}잔 주문되었습니다.\"\n","        full_summary = self.get_order_summary()\n","        response = f\"{latest_item_response} {full_summary}\"\n","\n","        # JSON output only for the latest item\n","        json_output = {\n","            \"action\": \"create_order\",\n","            \"order_items\": [item]\n","        }\n","        return response, json_output\n","\n","    def modify_order(self, old_drink, new_drink, temperature=None, size=None, add_ons=None, extra_shots=0):\n","        \"\"\"Modify an existing order item.\"\"\"\n","        # Locate the item in order history\n","        for item in self.order_history:\n","            if item[\"drink\"] == old_drink:\n","                # Update details\n","                item.update({\n","                    \"drink\": new_drink,\n","                    \"temperature\": temperature or item[\"temperature\"],\n","                    \"size\": size or item[\"size\"],\n","                    \"add_ons\": add_ons or item[\"add_ons\"],\n","                    \"extra_shots\": extra_shots or item[\"extra_shots\"]\n","                })\n","\n","                response = f\"주문이 {old_drink}에서 {new_drink}로 변경되었습니다. {self.get_order_summary()}\"\n","                json_output = {\n","                    \"action\": \"modify_order\",\n","                    \"old_drink\": old_drink,\n","                    \"new_drink\": new_drink,\n","                    \"temperature\": item[\"temperature\"],\n","                    \"size\": item[\"size\"],\n","                    \"quantity\": item[\"quantity\"],\n","                    \"add_ons\": item[\"add_ons\"],\n","                    \"extra_shots\": item[\"extra_shots\"]\n","                }\n","                return response, json_output\n","\n","        # If not found\n","        return f\"{old_drink}는 주문 내역에 없습니다.\", None\n","\n","    def get_order_summary(self):\n","        \"\"\"Generate a comprehensive summary of the current order history.\"\"\"\n","        if not self.order_history:\n","            return \"현재 주문된 항목이 없습니다.\"\n","\n","        summary = \"주문하신 내용은 다음과 같습니다:\"\n","        for item in self.order_history:\n","            drink_summary = (\n","                f\"- {item['drink']} {item['quantity']}잔 \"\n","                f\"({item['temperature']}, {item['size']}\"\n","            )\n","            if item['add_ons']:\n","                drink_summary += f\", 추가 옵션: {', '.join(item['add_ons'])}\"\n","            if item['extra_shots']:\n","                drink_summary += f\", 샷 추가: {item['extra_shots']}회\"\n","            drink_summary += \")\"\n","            summary += f\"\\n{drink_summary}\"\n","        return summary\n","\n","    def summarize_order_history(self):\n","        \"\"\"Provide a summary of the entire order history without adding new JSON output.\"\"\"\n","        if not self.order_history:\n","            return \"현재 주문된 항목이 없습니다.\", None\n","\n","        response = f\"지금까지의 주문내역입니다:\\n{self.get_order_summary()}\"\n","        return response, None\n","\n","    def process_input(self, input_text):\n","        \"\"\"Process customer input and route to the correct functionality.\"\"\"\n","        # Check for order completion\n","        if \"주문 완료할게\" in input_text:\n","            response = \"주문이 완료되었습니다. 결제는 카드리더기를 사용해주세요. 감사합니다.\"\n","            json_output = {\n","                \"action\": \"complete_order\",\n","                \"order_items\": self.order_history\n","            }\n","            self.reset_order()\n","            return response, json_output\n","\n","        if \"지금까지 뭘 주문했지\" in input_text:\n","            response = self.get_order_summary()\n","            json_output = None  # No JSON output for history summary\n","            return response, json_output\n","\n","        # Handle new order or modification based on input\n","        drink = None\n","        for item in self.menu_items + list(self.shorthand_mapping.keys()):\n","            if item in input_text:\n","                drink = self.shorthand_mapping.get(item, item)\n","                break\n","\n","        # If drink is not found, suggest alternatives\n","        if not drink:\n","            alternative = self.suggest_alternative(input_text)\n","            if alternative:\n","                response = f\"죄송합니다, {input_text}는 메뉴에 없습니다. 대신 {alternative}를 추천드립니다.\"\n","                json_output = {\n","                    \"action\": \"recommend_closest_item\",\n","                    \"requested_item\": input_text,\n","                    \"recommended_item\": alternative\n","                }\n","            else:\n","                response = f\"죄송합니다, {input_text}는 메뉴에 없습니다.\"\n","                json_output = None\n","            return response, json_output\n","\n","        # Process adding the new item to the order\n","        parsed_order = self.parse_order_input(input_text)\n","        if parsed_order:\n","            response, json_output = self.add_order_item(\n","                drink=parsed_order[\"drink\"],\n","                quantity=parsed_order.get(\"quantity\", 1),\n","                temperature=parsed_order.get(\"temperature\"),\n","                size=parsed_order.get(\"size\"),\n","                add_ons=parsed_order.get(\"add_ons\", []),\n","                extra_shots=parsed_order.get(\"extra_shots\", 0)\n","            )\n","            return response, json_output\n","\n","\n","    def parse_order_input(self, input_text):\n","        \"\"\"Parse input text to determine drink details.\"\"\"\n","        # Example parsing logic\n","        drink = None\n","        for item in self.menu_items:\n","            if item in input_text:\n","                drink = item\n","                break\n","\n","        if not drink:\n","            return None\n","\n","        quantity = 1\n","        if \"두잔\" in input_text:\n","            quantity = 2\n","        elif \"세잔\" in input_text:\n","            quantity = 3\n","\n","        temperature = \"아이스\" if \"아이스\" in input_text else None\n","        size = \"라지\" if \"라지\" in input_text else None\n","\n","        return {\n","            \"drink\": drink,\n","            \"quantity\": quantity,\n","            \"temperature\": temperature,\n","            \"size\": size\n","        }\n","\n","# Initialize the coffee kiosk instance\n","kiosk = CoffeeKiosk()\n","\n","# Interactive loop to simulate continuous conversation\n","print(\"Coffee Kiosk initialized and ready to take orders.\")\n","while True:\n","    customer_input = input(\"Customer: \")\n","    response, json_output = kiosk.process_input(customer_input)\n","    print(\"Kiosk:\", response)\n","    if json_output:\n","        print(\"JSON Output:\", json.dumps(json_output, ensure_ascii=False, indent=2))\n","    if \"주문 완료할게\" in customer_input:\n","        break\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"a5Ad3UrIU2Ok","executionInfo":{"status":"error","timestamp":1730545157199,"user_tz":-540,"elapsed":124505,"user":{"displayName":"Coffee Kiosk","userId":"02539563182731550933"}},"outputId":"1e214a39-f529-4ea3-9d4b-3c1553e86c02"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Coffee Kiosk initialized and ready to take orders.\n","Customer: 아메리카노 한잔주세요\n","Kiosk: 아메리카노 1잔 주문되었습니다. 주문하신 내용은 다음과 같습니다:\n","- 아메리카노 1잔 (핫, 미디움)\n","JSON Output: {\n","  \"action\": \"create_order\",\n","  \"order_items\": [\n","    {\n","      \"drink\": \"아메리카노\",\n","      \"quantity\": 1,\n","      \"temperature\": \"핫\",\n","      \"size\": \"미디움\",\n","      \"add_ons\": [],\n","      \"extra_shots\": 0\n","    }\n","  ]\n","}\n","Customer:  에스프레소 두잔 시원하게 주실래요?\n","Kiosk: 에스프레소 2잔 주문되었습니다. 주문하신 내용은 다음과 같습니다:\n","- 아메리카노 1잔 (핫, 미디움)\n","- 에스프레소 2잔 (핫, 미디움)\n","JSON Output: {\n","  \"action\": \"create_order\",\n","  \"order_items\": [\n","    {\n","      \"drink\": \"에스프레소\",\n","      \"quantity\": 2,\n","      \"temperature\": \"핫\",\n","      \"size\": \"미디움\",\n","      \"add_ons\": [],\n","      \"extra_shots\": 0\n","    }\n","  ]\n","}\n","Customer: 핫 아메리카노 2잔이랑 아이스 아메리카노 3잔 주세요\n","Kiosk: 아메리카노 1잔 주문되었습니다. 주문하신 내용은 다음과 같습니다:\n","- 아메리카노 1잔 (핫, 미디움)\n","- 에스프레소 2잔 (핫, 미디움)\n","- 아메리카노 1잔 (아이스, 미디움)\n","JSON Output: {\n","  \"action\": \"create_order\",\n","  \"order_items\": [\n","    {\n","      \"drink\": \"아메리카노\",\n","      \"quantity\": 1,\n","      \"temperature\": \"아이스\",\n","      \"size\": \"미디움\",\n","      \"add_ons\": [],\n","      \"extra_shots\": 0\n","    }\n","  ]\n","}\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"Interrupted by user","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-7d176ec75a69>\u001b[0m in \u001b[0;36m<cell line: 211>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Coffee Kiosk initialized and ready to take orders.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     \u001b[0mcustomer_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Customer: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkiosk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustomer_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Kiosk:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}]},{"cell_type":"code","source":["# Import necessary libraries\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch\n","import json\n","\n","# Define our coffee kiosk class to handle orders, responses, and JSON generation\n","class CoffeeKiosk:\n","    def __init__(self):\n","        # Model name for Bllossom's Llama\n","        model_name = \"Bllossom/llama-3.2-Korean-Bllossom-3B\"\n","\n","        # Initialize the tokenizer and model\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n","\n","        # Available menu items\n","        self.menu_items = [\n","            \"아메리카노\", \"라떼\", \"카푸치노\", \"카페모카\", \"바닐라라떼\",\n","            \"에스프레소\", \"카라멜마끼아또\", \"허브티\", \"홍차\", \"초콜릿라떼\",\n","            \"레몬에이드\", \"복숭아아이스티\", \"딸기스무디\", \"망고스무디\",\n","            \"키위주스\", \"토마토주스\"\n","        ]\n","        self.order_history = []\n","        self.default_size = \"미디움\"\n","        self.default_temperature = \"핫\"\n","        self.shorthand_mapping = {\n","            \"아아\": \"아이스 아메리카노\",\n","            \"뜨아\": \"핫 아메리카노\"\n","        }\n","\n","    def reset_order(self):\n","        \"\"\"Reset the order history for a new customer session.\"\"\"\n","        self.order_history = []\n","\n","    def is_menu_item(self, drink_name):\n","        \"\"\"Check if a drink is on the menu, including shorthand names.\"\"\"\n","        return drink_name in self.menu_items or drink_name in self.shorthand_mapping\n","\n","    def suggest_alternative(self, drink_name):\n","        \"\"\"Suggest a similar item if requested item is unavailable.\"\"\"\n","        if drink_name == \"초코라떼\":\n","            return \"초콜릿라떼\"\n","        return None\n","\n","    def add_order_item(self, drink, quantity=1, temperature=None, size=None, add_ons=None, extra_shots=0):\n","        \"\"\"Add a new item to the order.\"\"\"\n","        # Map shorthand to full drink name if necessary\n","        drink = self.shorthand_mapping.get(drink, drink)\n","\n","        # Default values if not specified\n","        size = size or self.default_size\n","        temperature = temperature or self.default_temperature\n","        add_ons = add_ons or []\n","\n","        item = {\n","            \"drink\": drink,\n","            \"quantity\": quantity,\n","            \"temperature\": temperature,\n","            \"size\": size,\n","            \"add_ons\": add_ons,\n","            \"extra_shots\": extra_shots\n","        }\n","\n","        self.order_history.append(item)\n","\n","        latest_item_response = f\"{drink} {quantity}잔 주문되었습니다.\"\n","        full_summary = self.get_order_summary()\n","        response = f\"{latest_item_response} {full_summary}\"\n","\n","        # JSON output only for the latest item\n","        json_output = {\n","            \"action\": \"create_order\",\n","            \"order_items\": [item]\n","        }\n","        return response, json_output\n","\n","    def modify_order(self, old_drink, new_drink, temperature=None, size=None, add_ons=None, extra_shots=0):\n","        \"\"\"Modify an existing order item.\"\"\"\n","        # Locate the item in order history\n","        for item in self.order_history:\n","            if item[\"drink\"] == old_drink:\n","                # Update details\n","                item.update({\n","                    \"drink\": new_drink,\n","                    \"temperature\": temperature or item[\"temperature\"],\n","                    \"size\": size or item[\"size\"],\n","                    \"add_ons\": add_ons or item[\"add_ons\"],\n","                    \"extra_shots\": extra_shots or item[\"extra_shots\"]\n","                })\n","\n","                response = f\"주문이 {old_drink}에서 {new_drink}로 변경되었습니다. {self.get_order_summary()}\"\n","                json_output = {\n","                    \"action\": \"modify_order\",\n","                    \"old_drink\": old_drink,\n","                    \"new_drink\": new_drink,\n","                    \"temperature\": item[\"temperature\"],\n","                    \"size\": item[\"size\"],\n","                    \"quantity\": item[\"quantity\"],\n","                    \"add_ons\": item[\"add_ons\"],\n","                    \"extra_shots\": item[\"extra_shots\"]\n","                }\n","                return response, json_output\n","\n","        # If not found\n","        return f\"{old_drink}는 주문 내역에 없습니다.\", None\n","\n","    def get_order_summary(self):\n","      \"\"\"Generate a comprehensive summary of the current order history.\"\"\"\n","      if not self.order_history:\n","        return \"현재 주문된 항목이 없습니다.\"\n","\n","      summary = \"주문하신 내용은 다음과 같습니다:\"\n","      for item in self.order_history:\n","            drink_summary = (\n","                f\"- {item['drink']} {item['quantity']}잔 \"\n","                f\"({item['temperature']}, {item['size']}\"\n","            )\n","            if item['add_ons']:\n","                drink_summary += f\", 추가 옵션: {', '.join(item['add_ons'])}\"\n","            if item['extra_shots']:\n","                drink_summary += f\", 샷 추가: {item['extra_shots']}회\"\n","            drink_summary += \")\"\n","            summary += f\"\\n{drink_summary}\"\n","      return summary\n","\n","    def summarize_order_history(self):\n","        \"\"\"Provide a summary of the entire order history without adding new JSON output.\"\"\"\n","        if not self.order_history:\n","           return \"현재 주문된 항목이 없습니다.\", None\n","\n","           response = f\"지금까지의 주문내역입니다:\\n{self.get_order_summary()}\"\n","           return response, None\n","\n","    def process_input(self, input_text):\n","        \"\"\"Process customer input and route to the correct functionality.\"\"\"\n","        # Check for order completion\n","        if \"주문 완료할게\" in input_text:\n","            response = \"주문이 완료되었습니다. 결제는 카드리더기를 사용해주세요. 감사합니다.\"\n","            json_output = {\n","                \"action\": \"complete_order\",\n","                \"order_items\": self.order_history\n","            }\n","            self.reset_order()\n","            return response, json_output\n","\n","        if \"지금까지 뭘 주문했지\" in input_text:\n","            response = self.get_order_summary()\n","            json_output = None  # No JSON output for history summary\n","            return response, json_output\n","\n","        # Handle new order or modification based on input\n","        drink = None\n","        for item in self.menu_items + list(self.shorthand_mapping.keys()):\n","            if item in input_text:\n","                drink = self.shorthand_mapping.get(item, item)\n","                break\n","\n","        # If drink is not found, suggest alternatives\n","        if not drink:\n","            alternative = self.suggest_alternative(input_text)\n","            if alternative:\n","                response = f\"죄송합니다, {input_text}는 메뉴에 없습니다. 대신 {alternative}를 추천드립니다.\"\n","                json_output = {\n","                    \"action\": \"recommend_closest_item\",\n","                    \"requested_item\": input_text,\n","                    \"recommended_item\": alternative\n","                }\n","            else:\n","                response = f\"죄송합니다, {input_text}는 메뉴에 없습니다.\"\n","                json_output = None\n","            return response, json_output\n","\n","        # Process adding the new item to the order\n","        parsed_order = self.parse_order_input(input_text)\n","        if parsed_order:\n","            response, json_output = self.add_order_item(\n","                drink=parsed_order[\"drink\"],\n","                quantity=parsed_order.get(\"quantity\", 1),\n","                temperature=parsed_order.get(\"temperature\"),\n","                size=parsed_order.get(\"size\"),\n","                add_ons=parsed_order.get(\"add_ons\", []),\n","                extra_shots=parsed_order.get(\"extra_shots\", 0)\n","            )\n","            return response, json_output\n","\n","\n","    def parse_order_input(self, input_text):\n","        \"\"\"Parse input text to determine drink details.\"\"\"\n","        # Example parsing logic\n","        drink = None\n","        for item in self.menu_items:\n","            if item in input_text:\n","                drink = item\n","                break\n","\n","        if not drink:\n","            return None\n","\n","        quantity = 1\n","        if \"두잔\" in input_text:\n","            quantity = 2\n","        elif \"세잔\" in input_text:\n","            quantity = 3\n","\n","        temperature = \"아이스\" if \"아이스\" in input_text else None\n","        size = \"라지\" if \"라지\" in input_text else None\n","\n","        return {\n","            \"drink\": drink,\n","            \"quantity\": quantity,\n","            \"temperature\": temperature,\n","            \"size\": size\n","        }\n","\n","    # Initialize the coffee kiosk instance\n","kiosk = CoffeeKiosk()\n","\n","# Interactive loop to simulate continuous conversation\n","print(\"Coffee Kiosk initialized and ready to take orders.\")\n","while True:\n","    customer_input = input(\"Customer: \")\n","    response, json_output = kiosk.process_input(customer_input)\n","    print(\"Kiosk:\", response)\n","    if json_output:\n","        print(\"JSON Output:\", json.dumps(json_output, ensure_ascii=False, indent=2))\n","    if \"주문 완료할게\" in customer_input:\n","        break\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["0a01999060184b2cb2f7fa49fc7ea2ca","2d68e76fd3de44fdbdf70ce4f4cf56a5","f33ae5b3e1bd48fc8670b85f80ab3c2e","e70394190082444e9015ccb2d24c042d","5703d63fed2a4a89baec743a8e62ee88","436f4603c40b4621a123d96cf3065248","396a9b55609548168490e4d74c5cc52c","8c6b583a61ff41f593abdd566ef82bcc","823d5f70ef1f40da85d16ba7852d6474","417b3d9c87434dd0a6391bc19f0738f6","0fb9b775ba6c4e7b8fed957c73de8a0d"]},"id":"AHROVbl5iLSf","outputId":"9e75ae89-84fe-425c-936e-d9c93de2a87d","executionInfo":{"status":"error","timestamp":1730545951964,"user_tz":-540,"elapsed":163858,"user":{"displayName":"Coffee Kiosk","userId":"02539563182731550933"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a01999060184b2cb2f7fa49fc7ea2ca"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Coffee Kiosk initialized and ready to take orders.\n","Customer: 카페라떼 라지로 2잔 주세요\n","Kiosk: 라떼 1잔 주문되었습니다. 주문하신 내용은 다음과 같습니다:\n","- 라떼 1잔 (핫, 라지)\n","JSON Output: {\n","  \"action\": \"create_order\",\n","  \"order_items\": [\n","    {\n","      \"drink\": \"라떼\",\n","      \"quantity\": 1,\n","      \"temperature\": \"핫\",\n","      \"size\": \"라지\",\n","      \"add_ons\": [],\n","      \"extra_shots\": 0\n","    }\n","  ]\n","}\n","Customer: 에스프레소 두잔 시원하게 주실래요?\n","Kiosk: 에스프레소 2잔 주문되었습니다. 주문하신 내용은 다음과 같습니다:\n","- 라떼 1잔 (핫, 라지)\n","- 에스프레소 2잔 (핫, 미디움)\n","JSON Output: {\n","  \"action\": \"create_order\",\n","  \"order_items\": [\n","    {\n","      \"drink\": \"에스프레소\",\n","      \"quantity\": 2,\n","      \"temperature\": \"핫\",\n","      \"size\": \"미디움\",\n","      \"add_ons\": [],\n","      \"extra_shots\": 0\n","    }\n","  ]\n","}\n","Customer: 아메리카노 한잔 주시고 카라멜마끼아또도 엑스라지로 한잔만 주세요\n","Kiosk: 아메리카노 1잔 주문되었습니다. 주문하신 내용은 다음과 같습니다:\n","- 라떼 1잔 (핫, 라지)\n","- 에스프레소 2잔 (핫, 미디움)\n","- 아메리카노 1잔 (핫, 라지)\n","JSON Output: {\n","  \"action\": \"create_order\",\n","  \"order_items\": [\n","    {\n","      \"drink\": \"아메리카노\",\n","      \"quantity\": 1,\n","      \"temperature\": \"핫\",\n","      \"size\": \"라지\",\n","      \"add_ons\": [],\n","      \"extra_shots\": 0\n","    }\n","  ]\n","}\n","Customer: 초콜릿라떼 두잔이랑 바닐라라떼 한잔 주세요\n","Kiosk: 라떼 2잔 주문되었습니다. 주문하신 내용은 다음과 같습니다:\n","- 라떼 1잔 (핫, 라지)\n","- 에스프레소 2잔 (핫, 미디움)\n","- 아메리카노 1잔 (핫, 라지)\n","- 라떼 2잔 (핫, 미디움)\n","JSON Output: {\n","  \"action\": \"create_order\",\n","  \"order_items\": [\n","    {\n","      \"drink\": \"라떼\",\n","      \"quantity\": 2,\n","      \"temperature\": \"핫\",\n","      \"size\": \"미디움\",\n","      \"add_ons\": [],\n","      \"extra_shots\": 0\n","    }\n","  ]\n","}\n","Customer: 아이스 아메리카노 4잔이랑 카페라떼 라지로 2잔 아이스 카라멜마끼아또 엑스라지로 휘핑크림 추가해서 3잔 주세요\n","Kiosk: 아메리카노 1잔 주문되었습니다. 주문하신 내용은 다음과 같습니다:\n","- 라떼 1잔 (핫, 라지)\n","- 에스프레소 2잔 (핫, 미디움)\n","- 아메리카노 1잔 (핫, 라지)\n","- 라떼 2잔 (핫, 미디움)\n","- 아메리카노 1잔 (아이스, 라지)\n","JSON Output: {\n","  \"action\": \"create_order\",\n","  \"order_items\": [\n","    {\n","      \"drink\": \"아메리카노\",\n","      \"quantity\": 1,\n","      \"temperature\": \"아이스\",\n","      \"size\": \"라지\",\n","      \"add_ons\": [],\n","      \"extra_shots\": 0\n","    }\n","  ]\n","}\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"Interrupted by user","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-c2302a7b7673>\u001b[0m in \u001b[0;36m<cell line: 220>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Coffee Kiosk initialized and ready to take orders.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0mcustomer_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Customer: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkiosk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustomer_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Kiosk:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}]},{"cell_type":"code","source":["# Import necessary libraries\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch\n","import json\n","\n","class CoffeeKiosk:\n","    def __init__(self):\n","        # Model setup for Llama\n","        model_name = \"Bllossom/llama-3.2-Korean-Bllossom-3B\"\n","\n","        # Load tokenizer\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","        # Set device to GPU if available, otherwise CPU\n","        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","        # Load model and move to specified device\n","        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(self.device)\n","\n","\n","        # Menu items and shorthand mappings\n","        self.menu_items = [\n","            \"아메리카노\", \"라떼\", \"카푸치노\", \"카페모카\", \"바닐라라떼\",\n","            \"에스프레소\", \"카라멜마끼아또\", \"허브티\", \"홍차\", \"초콜릿라떼\",\n","            \"레몬에이드\", \"복숭아아이스티\", \"딸기스무디\", \"망고스무디\",\n","            \"키위주스\", \"토마토주스\"\n","        ]\n","        self.order_history = []\n","        self.default_size = \"미디움\"\n","        self.default_temperature = \"핫\"\n","        self.shorthand_mapping = {\"아아\": \"아이스 아메리카노\", \"뜨아\": \"핫 아메리카노\"}\n","\n","    def is_menu_item(self, drink_name):\n","        \"\"\"Check if a drink is on the menu, including shorthand names.\"\"\"\n","        return drink_name in self.menu_items or drink_name in self.shorthand_mapping\n","\n","    def suggest_alternative(self, drink_name):\n","        \"\"\"Suggest a similar item if requested item is unavailable.\"\"\"\n","        if drink_name == \"초코라떼\":\n","            return \"초콜릿라떼\"\n","        return None\n","\n","    def reset_order(self):\n","        \"\"\"Reset the order history for a new customer session.\"\"\"\n","        self.order_history = []\n","\n","    def generate_response_with_model(self, prompt):\n","        # Tokenize input and move input tensor to the device\n","        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n","\n","        # Generate output from the model\n","        output = self.model.generate(**inputs,max_new_tokens=100)\n","\n","        # Decode the output and return the generated response\n","        response = self.tokenizer.decode(output[0], skip_special_tokens=True)\n","        return response\n","    def parse_order_input(self, input_text):\n","        \"\"\"Parse input text to determine drink details.\"\"\"\n","        # Example parsing logic\n","        drink = None\n","        for item in self.menu_items:\n","            if item in input_text:\n","                drink = item\n","                break\n","\n","        if not drink:\n","            return None\n","\n","        quantity = 1\n","        if \"두잔\" in input_text:\n","            quantity = 2\n","        elif \"세잔\" in input_text:\n","            quantity = 3\n","\n","        temperature = \"아이스\" if \"아이스\" in input_text else None\n","        size = \"라지\" if \"라지\" in input_text else None\n","\n","        return {\n","            \"drink\": drink,\n","            \"quantity\": quantity,\n","            \"temperature\": temperature,\n","            \"size\": size\n","        }\n","\n","    def process_input(self, input_text):\n","        \"\"\"Process customer input and route to the correct functionality.\"\"\"\n","        # Check for order completion\n","        if \"주문 완료할게\" in input_text:\n","            response = \"주문이 완료되었습니다. 결제는 카드리더기를 사용해주세요. 감사합니다.\"\n","            json_output = {\n","                \"action\": \"complete_order\",\n","                \"order_items\": self.order_history\n","            }\n","            self.reset_order()\n","            return response, json_output\n","\n","        if \"지금까지 뭘 주문했지\" in input_text:\n","            response = self.get_order_summary()\n","            json_output = None  # No JSON output for history summary\n","            return response, json_output\n","\n","        # Handle new order or modification based on input\n","        drink = None\n","        for item in self.menu_items + list(self.shorthand_mapping.keys()):\n","            if item in input_text:\n","                drink = self.shorthand_mapping.get(item, item)\n","                break\n","\n","        # If drink is not found, suggest alternatives\n","        if not drink:\n","            alternative = self.suggest_alternative(input_text)\n","            if alternative:\n","                response = f\"죄송합니다, {input_text}는 메뉴에 없습니다. 대신 {alternative}를 추천드립니다.\"\n","                json_output = {\n","                    \"action\": \"recommend_closest_item\",\n","                    \"requested_item\": input_text,\n","                    \"recommended_item\": alternative\n","                }\n","            else:\n","                response = f\"죄송합니다, {input_text}는 메뉴에 없습니다.\"\n","                json_output = None\n","            return response, json_output\n","\n","        # Process adding the new item to the order\n","        parsed_order = self.parse_order_input(input_text)\n","        if parsed_order:\n","            response, json_output = self.add_order_item(\n","                drink=parsed_order[\"drink\"],\n","                quantity=parsed_order.get(\"quantity\", 1),\n","                temperature=parsed_order.get(\"temperature\"),\n","                size=parsed_order.get(\"size\"),\n","                add_ons=parsed_order.get(\"add_ons\", []),\n","                extra_shots=parsed_order.get(\"extra_shots\", 0)\n","            )\n","            return response, json_output\n","\n","\n","    def parse_llama_response(self, response_text):\n","        \"\"\"Parse Llama-generated text to determine drink details.\"\"\"\n","        # Extract drink, quantity, temperature, size, add-ons from the model's response\n","        drink = None\n","        for item in self.menu_items + list(self.shorthand_mapping.keys()):\n","            if item in response_text:\n","                drink = self.shorthand_mapping.get(item, item)\n","                break\n","\n","        if not drink:\n","            return None\n","\n","        # Parse quantity, temperature, size, add-ons based on Llama output\n","        quantity = 1\n","        if \"두잔\" in response_text:\n","            quantity = 2\n","        elif \"세잔\" in response_text:\n","            quantity = 3\n","        elif \"네잔\" in response_text:\n","            quantity = 4\n","\n","        temperature = \"아이스\" if \"아이스\" in response_text else \"핫\"\n","        size = \"라지\" if \"라지\" in response_text or \"엑스라지\" in response_text else self.default_size\n","        add_ons = [\"휘핑크림\"] if \"휘핑크림 추가\" in response_text else []\n","\n","        return {\n","            \"drink\": drink,\n","            \"quantity\": quantity,\n","            \"temperature\": temperature,\n","            \"size\": size,\n","            \"add_ons\": add_ons\n","        }\n","\n","    def add_order_item(self, drink, quantity=1, temperature=None, size=None, add_ons=None, extra_shots=0):\n","        \"\"\"Add a new item to the order.\"\"\"\n","        item = {\n","            \"drink\": drink,\n","            \"quantity\": quantity,\n","            \"temperature\": temperature or self.default_temperature,\n","            \"size\": size or self.default_size,\n","            \"add_ons\": add_ons or [],\n","            \"extra_shots\": extra_shots\n","        }\n","        self.order_history.append(item)\n","\n","        latest_item_response = f\"{drink} {quantity}잔 주문되었습니다.\"\n","        full_summary = self.get_order_summary()\n","        response = f\"{latest_item_response} {full_summary}\"\n","\n","        # JSON output only for the latest item\n","        json_output = {\n","            \"action\": \"create_order\",\n","            \"order_items\": [item]\n","        }\n","        return response, json_output\n","    def modify_order(self, old_drink, new_drink, temperature=None, size=None, add_ons=None, extra_shots=0):\n","        \"\"\"Modify an existing order item.\"\"\"\n","        # Locate the item in order history\n","        for item in self.order_history:\n","            if item[\"drink\"] == old_drink:\n","                # Update details\n","                item.update({\n","                    \"drink\": new_drink,\n","                    \"temperature\": temperature or item[\"temperature\"],\n","                    \"size\": size or item[\"size\"],\n","                    \"add_ons\": add_ons or item[\"add_ons\"],\n","                    \"extra_shots\": extra_shots or item[\"extra_shots\"]\n","                })\n","\n","                response = f\"주문이 {old_drink}에서 {new_drink}로 변경되었습니다. {self.get_order_summary()}\"\n","                json_output = {\n","                    \"action\": \"modify_order\",\n","                    \"old_drink\": old_drink,\n","                    \"new_drink\": new_drink,\n","                    \"temperature\": item[\"temperature\"],\n","                    \"size\": item[\"size\"],\n","                    \"quantity\": item[\"quantity\"],\n","                    \"add_ons\": item[\"add_ons\"],\n","                    \"extra_shots\": item[\"extra_shots\"]\n","                }\n","                return response, json_output\n","\n","        # If not found\n","        return f\"{old_drink}는 주문 내역에 없습니다.\", None\n","\n","    def get_order_summary(self):\n","        \"\"\"Generate a comprehensive summary of the current order history.\"\"\"\n","        if not self.order_history:\n","            return \"현재 주문된 항목이 없습니다.\"\n","\n","        summary = \"주문하신 내용은 다음과 같습니다:\"\n","        for item in self.order_history:\n","            drink_summary = (\n","                f\"- {item['drink']} {item['quantity']}잔 \"\n","                f\"({item['temperature']}, {item['size']}\"\n","            )\n","            if item['add_ons']:\n","                drink_summary += f\", 추가 옵션: {', '.join(item['add_ons'])}\"\n","            if item['extra_shots']:\n","                drink_summary += f\", 샷 추가: {item['extra_shots']}회\"\n","            drink_summary += \")\"\n","            summary += f\"\\n{drink_summary}\"\n","        return summary\n","\n","    def summarize_order_history(self):\n","        \"\"\"Provide a summary of the entire order history without adding new JSON output.\"\"\"\n","        if not self.order_history:\n","           return \"현재 주문된 항목이 없습니다.\", None\n","\n","           response = f\"지금까지의 주문내역입니다:\\n{self.get_order_summary()}\"\n","           return response, None\n","\n","\n","# Initialize Coffee Kiosk\n","kiosk = CoffeeKiosk()\n","\n","# Simulate conversation loop\n","print(\"Coffee Kiosk initialized and ready to take orders.\")\n","while True:\n","    customer_input = input(\"Customer: \")\n","    response, json_output = kiosk.process_input(customer_input)\n","    print(\"Kiosk:\", response)\n","    if json_output:\n","        print(\"JSON Output:\", json.dumps(json_output, ensure_ascii=False, indent=2))\n","    if \"주문 완료할게\" in customer_input:\n","        break\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"zow6skBUmX9C","outputId":"9f2e8c68-5749-40d8-d1ea-d39b9b874053","executionInfo":{"status":"error","timestamp":1730554629558,"user_tz":-540,"elapsed":472,"user":{"displayName":"Coffee Kiosk","userId":"02539563182731550933"}}},"execution_count":null,"outputs":[{"output_type":"error","ename":"Exception","evalue":"data did not match any variant of untagged enum ModelWrapper at line 1251003 column 3","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-bd53f7dc6635>\u001b[0m in \u001b[0;36m<cell line: 252>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;31m# Initialize Coffee Kiosk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m \u001b[0mkiosk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoffeeKiosk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;31m# Simulate conversation loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-bd53f7dc6635>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Load tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Set device to GPU if available, otherwise CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    895\u001b[0m                     \u001b[0;34mf\"Tokenizer class {tokenizer_class_candidate} does not exist or is not currently imported.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m                 )\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;31m# Otherwise we have to be creative.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2269\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2271\u001b[0;31m         return cls._from_pretrained(\n\u001b[0m\u001b[1;32m   2272\u001b[0m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2273\u001b[0m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2503\u001b[0m         \u001b[0;31m# Instantiate the tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2504\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2505\u001b[0;31m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2506\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2507\u001b[0m             raise OSError(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mfast_tokenizer_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfrom_slow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;31m# We have a serialization from tokenizers which let us directly build the backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mfast_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizerFast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfast_tokenizer_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mslow_tokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;31m# We need to convert a slow tokenizer to build the backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mException\u001b[0m: data did not match any variant of untagged enum ModelWrapper at line 1251003 column 3"]}]},{"cell_type":"code","source":["# Import necessary libraries\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch\n","import json\n","import re\n","\n","class CoffeeKiosk:\n","    def __init__(self):\n","        # Model setup for Llama\n","        model_name = \"Bllossom/llama-3.2-Korean-Bllossom-3B\"\n","\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","        # Set device to GPU if available, otherwise CPU\n","        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","        # Load model and move to specified device with trust_remote_code=True\n","        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(self.device)\n","\n","        # Menu items and shorthand mappings\n","        self.menu_items = [\n","            \"아메리카노\", \"라떼\", \"카푸치노\", \"카페모카\", \"바닐라라떼\",\n","            \"에스프레소\", \"카라멜마끼아또\", \"허브티\", \"홍차\", \"초콜릿라떼\",\n","            \"레몬에이드\", \"복숭아아이스티\", \"딸기스무디\", \"망고스무디\",\n","            \"키위주스\", \"토마토주스\"\n","        ]\n","        self.order_history = []\n","        self.default_size = \"미디움\"\n","        self.default_temperature = \"핫\"\n","        self.shorthand_mapping = {\"아아\": \"아이스 아메리카노\", \"뜨아\": \"핫 아메리카노\"}\n","\n","    def is_menu_item(self, drink_name):\n","        \"\"\"Check if a drink is on the menu, including shorthand names.\"\"\"\n","        return drink_name in self.menu_items or drink_name in self.shorthand_mapping\n","\n","    def suggest_alternative(self, drink_name):\n","        \"\"\"Suggest a similar item if requested item is unavailable.\"\"\"\n","        if drink_name == \"초코라떼\":\n","            return \"초콜릿라떼\"\n","        return None\n","\n","    def reset_order(self):\n","        \"\"\"Reset the order history for a new customer session.\"\"\"\n","        self.order_history = []\n","\n","    def generate_response_with_model(self, prompt):\n","        # Tokenize input and move input tensor to the device\n","        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n","\n","        # Remove 'token_type_ids' if it exists to avoid errors\n","        inputs.pop(\"token_type_ids\", None)\n","\n","        # Generate output from the model with a specified max_new_tokens\n","        output = self.model.generate(**inputs, max_new_tokens=100)\n","\n","        # Decode the output and return the generated response\n","        response = self.tokenizer.decode(output[0], skip_special_tokens=True)\n","        return response\n","\n","    def parse_order_input(self, input_text):\n","        \"\"\"Parse input text to determine drink details.\"\"\"\n","        # Example parsing logic\n","        drink = None\n","        for item in self.menu_items:\n","            if item in input_text:\n","                drink = item\n","                break\n","\n","        if not drink:\n","            return None\n","\n","        quantity = 1\n","        if \"두잔\" in input_text:\n","            quantity = 2\n","        elif \"세잔\" in input_text:\n","            quantity = 3\n","\n","        temperature = \"아이스\" if \"아이스\" in input_text else None\n","        size = \"라지\" if \"라지\" in input_text else None\n","\n","        return {\n","            \"drink\": drink,\n","            \"quantity\": quantity,\n","            \"temperature\": temperature,\n","            \"size\": size\n","        }\n","\n","    def parse_multiple_items(self, input_text):\n","        \"\"\"Parse multiple items in a single input for better handling.\"\"\"\n","        # Regex pattern to match drink items with quantity, temperature, and size\n","        pattern = r\"(아이스|핫)?\\s*([가-힣]+)\\s*(엑스라지|라지|미디움)?\\s*(\\d+)?잔?\"\n","        items = re.findall(pattern, input_text)\n","\n","        orders = []\n","        for temp, drink, size, qty_text in items:\n","            drink = self.shorthand_mapping.get(drink, drink)  # Expand shorthand if necessary\n","            quantity = int(qty_text) if qty_text else 1\n","            temperature = temp if temp else self.default_temperature\n","            size = size if size else self.default_size\n","\n","            # Only add items that are on the menu\n","            if drink in self.menu_items:\n","                orders.append({\n","                    \"drink\": drink,\n","                    \"quantity\": quantity,\n","                    \"temperature\": temperature,\n","                    \"size\": size,\n","                    \"add_ons\": [],\n","                    \"extra_shots\": 0\n","                })\n","\n","        return orders\n","\n","    def process_input(self, input_text):\n","        \"\"\"Process customer input by routing it through the model and parsing the response.\"\"\"\n","        # Generate response from the Llama model\n","        response = self.generate_response_with_model(input_text)\n","\n","        # Check if the user asked for order completion\n","        if \"주문 완료할게\" in response:\n","            final_response = \"주문이 완료되었습니다. 결제는 카드리더기를 사용해주세요. 감사합니다.\"\n","            json_output = {\n","                \"action\": \"complete_order\",\n","                \"order_items\": self.order_history\n","            }\n","            self.reset_order()\n","            return final_response, json_output\n","\n","        # Check if the user asked for a summary of current orders\n","        if \"지금까지 뭘 주문했지\" in response:\n","            order_summary = self.get_order_summary()\n","            return order_summary, None\n","\n","        # Parse the response for adding or modifying orders\n","        parsed_orders = self.parse_multiple_items(input_text)\n","\n","        if parsed_orders:\n","            # If parsed orders are found, add each item to order history\n","            for parsed_order in parsed_orders:\n","                self.order_history.append(parsed_order)\n","\n","            # Generate the response and JSON for the cumulative order\n","            response = self.create_order_summary_response(parsed_orders)\n","            json_output = {\n","                \"action\": \"create_order\",\n","                \"order_items\": parsed_orders\n","            }\n","            return response, json_output\n","\n","        # Handle change requests, if detected in the input text\n","        old_drink, new_drink = self.extract_change_request(input_text)\n","        if old_drink and new_drink:\n","            response, json_output = self.modify_order(old_drink, new_drink)\n","            return response, json_output\n","\n","        # Fallback if no specific action is determined\n","        return response, None\n","\n","    def extract_change_request(self, input_text):\n","        \"\"\"Identify drinks to change and new drink preferences.\"\"\"\n","        match = re.search(r\"(\\S+)\\s+.*?를\\s+(\\S+)\\s+로\\s+바꿔줘\", input_text)\n","        if match:\n","            old_drink, new_drink = match.groups()\n","            return old_drink, new_drink\n","        return None, None\n","\n","    def add_order_item(self, drink, quantity=1, temperature=None, size=None, add_ons=None, extra_shots=0):\n","        \"\"\"Add a new item to the order.\"\"\"\n","        item = {\n","            \"drink\": drink,\n","            \"quantity\": quantity,\n","            \"temperature\": temperature or self.default_temperature,\n","            \"size\": size or self.default_size,\n","            \"add_ons\": add_ons or [],\n","            \"extra_shots\": extra_shots\n","        }\n","        self.order_history.append(item)\n","\n","        latest_item_response = f\"{drink} {quantity}잔 주문되었습니다.\"\n","        full_summary = self.get_order_summary()\n","        response = f\"{latest_item_response} {full_summary}\"\n","\n","        # JSON output only for the latest item\n","        json_output = {\n","            \"action\": \"create_order\",\n","            \"order_items\": [item]\n","        }\n","        return response, json_output\n","    def parse_llama_response(self, response_text):\n","        \"\"\"Parse Llama-generated text to determine drink details.\"\"\"\n","        # Extract drink, quantity, temperature, size, add-ons from the model's response\n","        drink = None\n","        for item in self.menu_items + list(self.shorthand_mapping.keys()):\n","            if item in response_text:\n","                drink = self.shorthand_mapping.get(item, item)\n","                break\n","\n","        if not drink:\n","            return None\n","\n","        # Parse quantity, temperature, size, add-ons based on Llama output\n","        quantity = 1\n","        if \"두잔\" in response_text:\n","            quantity = 2\n","        elif \"세잔\" in response_text:\n","            quantity = 3\n","        elif \"네잔\" in response_text:\n","            quantity = 4\n","\n","        temperature = \"아이스\" if \"아이스\" in response_text else \"핫\"\n","        size = \"라지\" if \"라지\" in response_text or \"엑스라지\" in response_text else self.default_size\n","        add_ons = [\"휘핑크림\"] if \"휘핑크림 추가\" in response_text else []\n","\n","        return {\n","            \"drink\": drink,\n","            \"quantity\": quantity,\n","            \"temperature\": temperature,\n","            \"size\": size,\n","            \"add_ons\": add_ons\n","        }\n","\n","    def modify_order(self, old_drink, new_drink, temperature=None, size=None, add_ons=None, extra_shots=0):\n","        \"\"\"Modify an existing order item.\"\"\"\n","        # Locate the item in order history\n","        for item in self.order_history:\n","            if item[\"drink\"] == old_drink:\n","                # Update details\n","                item.update({\n","                    \"drink\": new_drink,\n","                    \"temperature\": temperature or item[\"temperature\"],\n","                    \"size\": size or item[\"size\"],\n","                    \"add_ons\": add_ons or item[\"add_ons\"],\n","                    \"extra_shots\": extra_shots or item[\"extra_shots\"]\n","                })\n","\n","                response = f\"주문이 {old_drink}에서 {new_drink}로 변경되었습니다. {self.get_order_summary()}\"\n","                json_output = {\n","                    \"action\": \"modify_order\",\n","                    \"old_drink\": old_drink,\n","                    \"new_drink\": new_drink,\n","                    \"temperature\": item[\"temperature\"],\n","                    \"size\": item[\"size\"],\n","                    \"quantity\": item[\"quantity\"],\n","                    \"add_ons\": item[\"add_ons\"],\n","                    \"extra_shots\": item[\"extra_shots\"]\n","                }\n","                return response, json_output\n","\n","        # If not found\n","        return f\"{old_drink}는 주문 내역에 없습니다.\", None\n","\n","    def get_order_summary(self):\n","        \"\"\"Generate a comprehensive summary of the current order history.\"\"\"\n","        if not self.order_history:\n","            return \"현재 주문된 항목이 없습니다.\"\n","\n","        summary = \"주문하신 내용은 다음과 같습니다:\"\n","        for item in self.order_history:\n","            drink_summary = (\n","                f\"- {item['drink']} {item['quantity']}잔 \"\n","                f\"({item['temperature']}, {item['size']}\"\n","            )\n","            if item['add_ons']:\n","                drink_summary += f\", 추가 옵션: {', '.join(item['add_ons'])}\"\n","            if item['extra_shots']:\n","                drink_summary += f\", 샷 추가: {item['extra_shots']}회\"\n","            drink_summary += \")\"\n","            summary += f\"\\n{drink_summary}\"\n","        return summary\n","\n","    def create_order_summary_response(self, parsed_orders):\n","        \"\"\"Create a summary response for the current and cumulative orders.\"\"\"\n","        latest_items = \", \".join(\n","            [f\"{item['drink']} {item['quantity']}잔 ({item['temperature']}, {item['size']})\" for item in parsed_orders]\n","        )\n","        latest_item_response = f\"{latest_items} 주문되었습니다.\"\n","\n","        # Generate a full summary of all items in the order history\n","        full_summary = self.get_order_summary()\n","        response = f\"{latest_item_response}\\n{full_summary}\"\n","\n","        return response\n","\n","    def summarize_order_history(self):\n","        \"\"\"Provide a summary of the entire order history without adding new JSON output.\"\"\"\n","        if not self.order_history:\n","           return \"현재 주문된 항목이 없습니다.\", None\n","\n","           response = f\"지금까지의 주문내역입니다:\\n{self.get_order_summary()}\"\n","           return response, None\n","\n","# Initialize Coffee Kiosk\n","kiosk = CoffeeKiosk()\n","\n","# Simulate conversation loop\n","print(\"Coffee Kiosk initialized and ready to take orders.\")\n","while True:\n","    customer_input = input(\"Customer: \")\n","    response, json_output = kiosk.process_input(customer_input)\n","    print(\"Kiosk:\", response)\n","    if json_output:\n","        print(\"JSON Output:\", json.dumps(json_output, ensure_ascii=False, indent=2))\n","    if \"주문 완료할게\" in customer_input:\n","        break\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["79c30191b7214cf598c2bb1a883a8cdb","555e650fd5544f838921fc993186b2f6","a67cd84efcec4d4896f41955e272c9ab","600d532ac4a144a7990d9fa22e6dd4b8","e8c5340032c748f58e3b452713f3cfd3","c96412a7509f4b7ea1d232feafe4f414","38f172da07c541bc9901d20ab0b8d50b","c54dba6d08624c89acc65026ec65fa4f","941532563bf84f09935623eb9e0772ac","b0c79001d814488c807dede50d9ec315","3b2a8691fb3043e59e4c07def420c428"]},"id":"v7kx7QyHwYg7","outputId":"354f8eaf-2442-451b-826e-6a78a1b6686e","executionInfo":{"status":"error","timestamp":1730557428326,"user_tz":-540,"elapsed":248318,"user":{"displayName":"Coffee Kiosk","userId":"02539563182731550933"}}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79c30191b7214cf598c2bb1a883a8cdb"}},"metadata":{}},{"name":"stdout","output_type":"stream","text":["Coffee Kiosk initialized and ready to take orders.\n","Customer: 아이스 아메리카노 6잔 주세요\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Kiosk: 아메리카노 6잔 (아이스, 미디움) 주문되었습니다.\n","주문하신 내용은 다음과 같습니다:\n","- 아메리카노 6잔 (아이스, 미디움)\n","JSON Output: {\n","  \"action\": \"create_order\",\n","  \"order_items\": [\n","    {\n","      \"drink\": \"아메리카노\",\n","      \"quantity\": 6,\n","      \"temperature\": \"아이스\",\n","      \"size\": \"미디움\",\n","      \"add_ons\": [],\n","      \"extra_shots\": 0\n","    }\n","  ]\n","}\n","Customer: 쿠키앤크림 3잔 주세요\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Kiosk: 쿠키앤크림 3잔 주세요! (제목: 쿠키앤크림 3잔 주세요!) - 네이트 뉴스\n","쿠키앤크림 3잔 주세요! (제목: 쿠키앤크림 3잔 주세요!)\n","\n","쿠키앤크림 3잔 주세요! (제목: 쿠키앤크림 3잔 주세요!)\n","\n","2023-04-05 10:\n","Customer: 카페 모카 2잔\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Kiosk: 카페 모카 2잔, 카페 모카 2잔, 카페 모카 2잔, 카페 모카 2잔, 카페 모카 2잔, 카페 모카 2잔, 카페 모카 2잔, 카페 모카 2잔, 카페 모카 2잔, 카페 모카 2잔, 카페 모카 2잔, 카페 모카 2잔, 카페 모카 2잔, 카페 모\n","Customer: 복숭아 아이스티 한잔 주세요\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Kiosk: 복숭아 아이스티 한잔 주세요! 복숭아 아이스티는 맛있는 음료로, 주로 여름철에 즐기기 좋습니다. 이 음료는 주로 복숭아의 맛을 뽐내며, 아이스와 함께 마시면 더욱 맛있게 느껴집니다. 복숭아 아이스티는 다양한 맛의 복숭아를 사용하여 다양한 hương을 즐길 수 있습니다. 또한, 복숭아 아이스티는 보다 건강한\n","Customer: 카페라때 한잔 주세요\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["Kiosk: 카페라때 한잔 주세요. (Café au lait, please.) - \"나의 아내는 한 잔을 마시기 전에 항상 한 잔 더 남겼습니다. 나는 그 아내가 나의 아내가 아니라는 것을 알았습니다. 나는 그 아내가 나의 아내가 아니라는 것을 알았습니다.\" - \"나는 그 아내가 나의 아내가 아니라는 것을 알았습니다. 나는 그 아내가 나의 아내가 아니라는 것을\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"Interrupted by user","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-50a0ee3ce795>\u001b[0m in \u001b[0;36m<cell line: 297>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Coffee Kiosk initialized and ready to take orders.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m     \u001b[0mcustomer_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Customer: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m     \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkiosk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustomer_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Kiosk:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}]},{"cell_type":"code","source":["!pip install --upgrade transformers\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uYHayJoOImhL","executionInfo":{"status":"ok","timestamp":1730559502319,"user_tz":-540,"elapsed":3305,"user":{"displayName":"Coffee Kiosk","userId":"02539563182731550933"}},"outputId":"8acc8b6b-98b7-4c62-cfbe-019b05676fe5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"]}]},{"cell_type":"code","source":["pip install tensorflow\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KUfyDQBWVQSk","executionInfo":{"status":"ok","timestamp":1730559374256,"user_tz":-540,"elapsed":2250,"user":{"displayName":"Coffee Kiosk","userId":"02539563182731550933"}},"outputId":"f482cca4-bd79-490e-b5c7-c7c0878333fb"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.9.1)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.12)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n","Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.9.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.1.2)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.19.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.9.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.9.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.7)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.32.3)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.0.4)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.5.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2024.8.30)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow) (2.1.5)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.2)\n"]}]},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch\n","import json\n","import re"],"metadata":{"id":"qxG_oGbfWQVq","executionInfo":{"status":"ok","timestamp":1730559463173,"user_tz":-540,"elapsed":2160,"user":{"displayName":"Coffee Kiosk","userId":"02539563182731550933"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["pip install tensorflow==2.9.1\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3pLN5BN5WiOr","executionInfo":{"status":"ok","timestamp":1730560143396,"user_tz":-540,"elapsed":19560,"user":{"displayName":"Coffee Kiosk","userId":"02539563182731550933"}},"outputId":"67c1c529-6e6b-45d7-c852-498953c6e4d4"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow==2.9.1\n","  Using cached tensorflow-2.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (1.6.3)\n","Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (1.12)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (1.64.1)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (3.11.0)\n","Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (2.9.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (1.1.2)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (18.1.1)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (2.1.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (24.1)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (3.19.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (71.0.4)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (1.16.0)\n","Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (2.9.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (0.37.1)\n","Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (2.9.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1) (1.16.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.9.1) (0.44.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.4.6)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.7)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.32.3)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.8.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.0.4)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (5.5.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2024.8.30)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.1.5)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.6.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.2.2)\n","Using cached tensorflow-2.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n","Installing collected packages: tensorflow\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed tensorflow-2.9.1\n"]}]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","import json\n","import re\n","\n","class CoffeeKiosk:\n","    def __init__(self):\n","        model_name = \"Bllossom/llama-3.2-Korean-Bllossom-3B\"\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(self.device)\n","\n","        # Menu and default configurations\n","        self.menu_items = [\n","            \"아메리카노\", \"라떼\", \"카푸치노\", \"카페모카\", \"바닐라라떼\",\n","            \"에스프레소\", \"카라멜마끼아또\", \"허브티\", \"홍차\", \"초콜릿라떼\",\n","            \"레몬에이드\", \"복숭아아이스티\", \"딸기스무디\", \"망고스무디\",\n","            \"키위주스\", \"토마토주스\"\n","        ]\n","        self.default_size = \"미디움\"\n","        self.default_temperature = \"핫\"\n","        self.shorthand_mapping = {\"아아\": \"아이스 아메리카노\", \"뜨아\": \"핫 아메리카노\"}\n","        self.order_history = []\n","\n","    def generate_response_with_model(self, input_text, parsed_orders):\n","        \"\"\"Generate a confirmation response based on parsed orders.\"\"\"\n","        # Create a specific prompt to guide the model without verbose instructions\n","        order_details = \", \".join([\n","            f\"{item['drink']} {item['quantity']}잔 ({item['temperature']}, {item['size']})\"\n","            for item in parsed_orders\n","        ])\n","        prompt = f\"{order_details} 주문되었습니다. 다음은 현재까지 주문 내역입니다:\\n{self.get_order_summary()}\"\n","\n","        # Tokenize prompt and generate output with controlled response length\n","        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n","        output = self.model.generate(**inputs, max_length=100, no_repeat_ngram_size=2)\n","        response = self.tokenizer.decode(output[0], skip_special_tokens=True).strip()\n","        return response\n","\n","    def is_valid_order(self, drink_name):\n","        \"\"\"Check if a drink is valid or mapped to a shorthand.\"\"\"\n","        return drink_name in self.menu_items or drink_name in self.shorthand_mapping\n","\n","    def handle_input(self, input_text):\n","        \"\"\"Process customer input by parsing and generating structured output.\"\"\"\n","        # Check if the customer asked to complete the order\n","        if \"주문 완료할게\" in input_text:\n","            final_response = \"주문이 완료되었습니다. 결제는 카드리더기를 사용해주세요. 감사합니다.\"\n","            json_output = {\n","                \"action\": \"complete_order\",\n","                \"order_items\": self.order_history\n","            }\n","            self.reset_order()\n","            return final_response, json_output\n","\n","        # Parse the input to extract multiple items if available\n","        parsed_orders = self.parse_multiple_items(input_text)\n","        if parsed_orders:\n","            # Add parsed items to order history\n","            for parsed_order in parsed_orders:\n","                self.order_history.append(parsed_order)\n","\n","            # Generate a model-based response and JSON for the latest input only\n","            response = self.generate_response_with_model(input_text, parsed_orders)\n","            json_output = {\n","                \"action\": \"create_order\",\n","                \"order_items\": parsed_orders\n","            }\n","            return response, json_output\n","\n","        # If no valid items were parsed, respond with a message\n","        return \"죄송합니다, 유효한 주문 항목이 없습니다. 메뉴에서 선택해주세요.\", None\n","\n","        # Construct prompt for the model\n","        prompt = f\"당신은 고객의 음료 주문을 처리하는 가상 커피 키오스크입니다. 다음 주문을 이해하고 요약하세요: '{input_text}'\"\n","        response = self.generate_response_with_model(prompt)\n","\n","        # Parse and update orders\n","        parsed_orders = self.parse_multiple_items(input_text)\n","        if parsed_orders:\n","            for parsed_order in parsed_orders:\n","                self.order_history.append(parsed_order)\n","            response = self.create_order_summary_response(parsed_orders)\n","            json_output = {\n","                \"action\": \"create_order\",\n","                \"order_items\": parsed_orders\n","            }\n","            return response, json_output\n","\n","        return response, None\n","\n","    def parse_multiple_items(self, input_text):\n","        \"\"\"Parse and validate multiple items.\"\"\"\n","        pattern = r\"(아이스|핫)?\\s*([가-힣]+)\\s*(엑스라지|라지|미디움)?\\s*(\\d+)?잔?\"\n","        items = re.findall(pattern, input_text)\n","        orders = []\n","\n","        for temp, drink, size, qty_text in items:\n","            drink = self.shorthand_mapping.get(drink, drink)\n","            quantity = int(qty_text) if qty_text else 1\n","            temperature = temp if temp else self.default_temperature\n","            size = size if size else self.default_size\n","\n","            if self.is_valid_order(drink):\n","                orders.append({\n","                    \"drink\": drink,\n","                    \"quantity\": quantity,\n","                    \"temperature\": temperature,\n","                    \"size\": size,\n","                    \"add_ons\": [],\n","                    \"extra_shots\": 0\n","                })\n","        return orders\n","\n","    def create_order_summary_response(self, parsed_orders):\n","        latest_items = \", \".join(\n","            [f\"{item['drink']} {item['quantity']}잔 ({item['temperature']}, {item['size']})\" for item in parsed_orders]\n","        )\n","        latest_item_response = f\"{latest_items} 주문되었습니다.\"\n","        full_summary = self.get_order_summary()\n","        response = f\"{latest_item_response}\\n{full_summary}\"\n","        return response\n","\n","    def get_order_summary(self):\n","        \"\"\"Provide a summary of all orders in a readable format.\"\"\"\n","        if not self.order_history:\n","            return \"현재 주문된 항목이 없습니다.\"\n","\n","        summary = \"주문하신 내용은 다음과 같습니다:\"\n","        for item in self.order_history:\n","            drink_summary = (\n","                f\"- {item['drink']} {item['quantity']}잔 \"\n","                f\"({item['temperature']}, {item['size']}\"\n","            )\n","            if item['add_ons']:\n","                drink_summary += f\", 추가 옵션: {', '.join(item['add_ons'])}\"\n","            if item['extra_shots']:\n","                drink_summary += f\", 샷 추가: {item['extra_shots']}회\"\n","            drink_summary += \")\"\n","            summary += f\"\\n{drink_summary}\"\n","        return summary\n","\n","# Initialize Coffee Kiosk\n","kiosk = CoffeeKiosk()\n","print(\"Coffee Kiosk initialized and ready to take orders.\")\n","\n","# Simulate conversation loop\n","while True:\n","    customer_input = input(\"Customer: \")\n","    response, json_output = kiosk.handle_input(customer_input)\n","    print(\"Kiosk:\", response)\n","    if json_output:\n","        print(\"JSON Output:\", json.dumps(json_output, ensure_ascii=False, indent=2))\n","    if \"주문 완료할게\" in customer_input:\n","        break\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"-edFv_ACSgwQ","executionInfo":{"status":"error","timestamp":1730560152325,"user_tz":-540,"elapsed":3324,"user":{"displayName":"Coffee Kiosk","userId":"02539563182731550933"}},"outputId":"4366cdfe-1add-4fbb-82c5-7664dacdf3c5"},"execution_count":11,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):\nUnable to convert function return value to a Python type! The signature was\n\t() -> handle","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1777\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mmodeling_rope_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mROPE_INIT_FUNCTIONS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mmodeling_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mpytorch_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mALL_LAYERNORM_LAYERS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mintegrations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPeftAdapterMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepspeed_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_deepspeed_zero3_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLOSS_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m from .pytorch_utils import (  # noqa: F401\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/loss/loss_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss_deformable_detr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeformableDetrForObjectDetectionLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeformableDetrForSegmentationLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss_for_object_detection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mForObjectDetectionLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForSegmentationLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/loss/loss_deformable_detr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_transforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcenter_to_corners_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_scipy_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/image_transforms.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Bring in subpackages.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/experimental/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mservice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatching\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdense_to_ragged_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/experimental/service/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrom_dataset_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/experimental/ops/data_service_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompression_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pywrap_server_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/experimental/ops/compression_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"Ops for compressing and uncompressing dataset elements.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_experimental_dataset_ops\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mged_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcomposite_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/util/nest.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse_tensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sparse_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pywrap_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/sparse_tensor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcomposite_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/dtypes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0m_np_bfloat16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pywrap_bfloat16\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_bfloat16_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Unable to convert function return value to a Python type! The signature was\n\t() -> handle","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-82c68d9b36db>\u001b[0m in \u001b[0;36m<cell line: 144>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;31m# Initialize Coffee Kiosk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m \u001b[0mkiosk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoffeeKiosk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Coffee Kiosk initialized and ready to take orders.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-82c68d9b36db>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Menu and default configurations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m             )\n\u001b[1;32m    562\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m             return model_class.from_pretrained(\n\u001b[1;32m    565\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m_get_model_class\u001b[0;34m(config, model_mapping)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m     \u001b[0msupported_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupported_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msupported_models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_attr_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;31m# Maybe there was several model types associated with this config.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m_load_attr_from_module\u001b[0;34m(self, model_type, attr)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodule_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\".{module_name}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transformers.models\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattribute_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattribute_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m     \u001b[0;31m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1764\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlaceholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1765\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1766\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1767\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1768\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1778\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1779\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1780\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1781\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):\nUnable to convert function return value to a Python type! The signature was\n\t() -> handle"]}]},{"cell_type":"code","source":["!pip install -U transformers torch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"lgqNoBVvbL6M","executionInfo":{"status":"ok","timestamp":1730560328330,"user_tz":-540,"elapsed":124836,"user":{"displayName":"Coffee Kiosk","userId":"02539563182731550933"}},"outputId":"f776c118-959e-4bdf-9206-e6b6a2ad644f"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n","Collecting torch\n","  Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2.1.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n","  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting triton==3.1.0 (from torch)\n","  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n","Collecting sympy==1.13.1 (from torch)\n","  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n","Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m488.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n","  Attempting uninstall: sympy\n","    Found existing installation: sympy 1.13.3\n","    Uninstalling sympy-1.13.3:\n","      Successfully uninstalled sympy-1.13.3\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.23.4\n","    Uninstalling nvidia-nccl-cu12-2.23.4:\n","      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.4.1+cu121\n","    Uninstalling torch-2.4.1+cu121:\n","      Successfully uninstalled torch-2.4.1+cu121\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","fastai 2.7.17 requires torch<2.5,>=1.10, but you have torch 2.5.1 which is incompatible.\n","torchaudio 2.4.1+cu121 requires torch==2.4.1, but you have torch 2.5.1 which is incompatible.\n","torchvision 0.19.1+cu121 requires torch==2.4.1, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.5.1 triton-3.1.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["sympy","torch","torchgen"]},"id":"c2e73b7d03894642bb3ea645a83fd3b6"}},"metadata":{}}]},{"cell_type":"code","source":["!pip uninstall -y tensorflow\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W57MueY1afcC","executionInfo":{"status":"ok","timestamp":1730560009482,"user_tz":-540,"elapsed":1860,"user":{"displayName":"Coffee Kiosk","userId":"02539563182731550933"}},"outputId":"e6161cd9-d83e-477f-c108-6e92879a882a"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: tensorflow 2.9.1\n","Uninstalling tensorflow-2.9.1:\n","  Successfully uninstalled tensorflow-2.9.1\n"]}]},{"cell_type":"code","source":["!pip install --upgrade --force-reinstall pandas\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"9kM3hETuZiTm","executionInfo":{"status":"ok","timestamp":1730559774997,"user_tz":-540,"elapsed":22424,"user":{"displayName":"Coffee Kiosk","userId":"02539563182731550933"}},"outputId":"45eb0973-ca35-49e1-92ee-4971ba8995ed"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pandas\n","  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting numpy>=1.22.4 (from pandas)\n","  Downloading numpy-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting python-dateutil>=2.8.2 (from pandas)\n","  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n","Collecting pytz>=2020.1 (from pandas)\n","  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n","Collecting tzdata>=2022.7 (from pandas)\n","  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n","Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas)\n","  Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n","Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hUsing cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n","Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hUsing cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n","Installing collected packages: pytz, tzdata, six, numpy, python-dateutil, pandas\n","  Attempting uninstall: pytz\n","    Found existing installation: pytz 2024.2\n","    Uninstalling pytz-2024.2:\n","      Successfully uninstalled pytz-2024.2\n","  Attempting uninstall: tzdata\n","    Found existing installation: tzdata 2024.1\n","    Uninstalling tzdata-2024.1:\n","      Successfully uninstalled tzdata-2024.1\n","  Attempting uninstall: six\n","    Found existing installation: six 1.16.0\n","    Uninstalling six-1.16.0:\n","      Successfully uninstalled six-1.16.0\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.26.4\n","    Uninstalling numpy-1.26.4:\n","      Successfully uninstalled numpy-1.26.4\n","  Attempting uninstall: python-dateutil\n","    Found existing installation: python-dateutil 2.8.2\n","    Uninstalling python-dateutil-2.8.2:\n","      Successfully uninstalled python-dateutil-2.8.2\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 2.1.4\n","    Uninstalling pandas-2.1.4:\n","      Successfully uninstalled pandas-2.1.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires numpy<2.0a0,>=1.23, but you have numpy 2.1.2 which is incompatible.\n","cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\n","cudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 3.19.6 which is incompatible.\n","cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.1.2 which is incompatible.\n","gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.1.2 which is incompatible.\n","google-colab 1.0.0 requires pandas==2.1.4, but you have pandas 2.2.3 which is incompatible.\n","ibis-framework 8.0.0 requires numpy<2,>=1, but you have numpy 2.1.2 which is incompatible.\n","numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.2 which is incompatible.\n","pandas-gbq 0.23.1 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n","pytensor 2.25.4 requires numpy<2,>=1.17.0, but you have numpy 2.1.2 which is incompatible.\n","rmm-cu12 24.4.0 requires numpy<2.0a0,>=1.23, but you have numpy 2.1.2 which is incompatible.\n","tensorflow-datasets 4.9.6 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n","tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.9.1 which is incompatible.\n","thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-2.1.2 pandas-2.2.3 python-dateutil-2.9.0.post0 pytz-2024.2 six-1.16.0 tzdata-2024.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["dateutil","pytz","six"]},"id":"58fbf3ddbcb344eeb0019e3d996027f8"}},"metadata":{}}]},{"cell_type":"code","source":["# 트랜스포머\n","!pip install transformers\n","!pip install torch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RG4cGgC-ZTuH","executionInfo":{"status":"ok","timestamp":1730559913186,"user_tz":-540,"elapsed":4369,"user":{"displayName":"Coffee Kiosk","userId":"02539563182731550933"}},"outputId":"654ab601-7477-4d73-d54a-790152dd9a7a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2.1.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"]}]},{"cell_type":"code","source":["!pip install --upgrade transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nvhYfnp_ZHlw","executionInfo":{"status":"ok","timestamp":1730559919288,"user_tz":-540,"elapsed":3276,"user":{"displayName":"Coffee Kiosk","userId":"02539563182731550933"}},"outputId":"f54a55f6-876a-481f-ced4-da599fa467c8"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2.1.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"]}]}]}