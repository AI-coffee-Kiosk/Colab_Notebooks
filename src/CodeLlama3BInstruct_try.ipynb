{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"22b3ed040e2549e9954a03f0fe45a467":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e3b7108077a04212b08ea85df0e6366c","IPY_MODEL_6844331022e941e094a4a8a295c9ad5b","IPY_MODEL_0685a0dd67ad4de5a3e2fe7367866cbd"],"layout":"IPY_MODEL_61ad9512d4974a809499e77263b63890"}},"e3b7108077a04212b08ea85df0e6366c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_17cf1035d1a345529cc635f60ea0a8cc","placeholder":"â€‹","style":"IPY_MODEL_3677f4b3010e4cb88a034bfb8f713f0f","value":"Map:â€‡100%"}},"6844331022e941e094a4a8a295c9ad5b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_217662080c0146fdb2ca05798bfb2b05","max":319,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3c1e011bcf4845d397f66bdcb032e769","value":319}},"0685a0dd67ad4de5a3e2fe7367866cbd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1188114d4f1946fba6cd7550f84d703c","placeholder":"â€‹","style":"IPY_MODEL_fe4f96a23b1746c6ae917d76213d6901","value":"â€‡319/319â€‡[00:00&lt;00:00,â€‡7047.62â€‡examples/s]"}},"61ad9512d4974a809499e77263b63890":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17cf1035d1a345529cc635f60ea0a8cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3677f4b3010e4cb88a034bfb8f713f0f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"217662080c0146fdb2ca05798bfb2b05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c1e011bcf4845d397f66bdcb032e769":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1188114d4f1946fba6cd7550f84d703c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe4f96a23b1746c6ae917d76213d6901":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52743d238871425496b62710ba98b37f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1f9567e2c10e4eb79c8251c3c79c629b","IPY_MODEL_b133a0a636c94bbab0d399d5c131d8d4","IPY_MODEL_483b8f47ee8e4d8ebfd8b0a2d26c6f39"],"layout":"IPY_MODEL_95e98e80422d468ca6c27ca3c0b8afc2"}},"1f9567e2c10e4eb79c8251c3c79c629b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac3d391b838940bf9bec60cb77439cc7","placeholder":"â€‹","style":"IPY_MODEL_7a8af183326a4074b3b2c249faec7828","value":"Loadingâ€‡checkpointâ€‡shards:â€‡100%"}},"b133a0a636c94bbab0d399d5c131d8d4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8dd72e11c0044469686ef0553b5d901","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ddd48f0fe09444379787ed7bd82ec795","value":2}},"483b8f47ee8e4d8ebfd8b0a2d26c6f39":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_957bfbdcdec04748adb93cb60d1f2c0f","placeholder":"â€‹","style":"IPY_MODEL_51de19d9f6774d0aa41e65961f3fb7a2","value":"â€‡2/2â€‡[00:03&lt;00:00,â€‡â€‡1.55s/it]"}},"95e98e80422d468ca6c27ca3c0b8afc2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac3d391b838940bf9bec60cb77439cc7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a8af183326a4074b3b2c249faec7828":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f8dd72e11c0044469686ef0553b5d901":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ddd48f0fe09444379787ed7bd82ec795":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"957bfbdcdec04748adb93cb60d1f2c0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51de19d9f6774d0aa41e65961f3fb7a2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"HzdOb0GTI5gd","collapsed":true},"outputs":[],"source":["!pip install transformers\n","!pip install torch\n","!pip install huggingface_hub"]},{"cell_type":"code","source":["!pip install --upgrade transformers\n","!pip install --upgrade torch"],"metadata":{"id":"eP68ukf1MK_V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prompt to enter your Hugging Face token securely\n","import os\n","os.environ[\"HF_TOKEN\"] = input(\"Enter your Hugging Face token: \")"],"metadata":{"id":"C0nw7Zt1Nspt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from huggingface_hub import login\n","login(token=os.getenv(\"HF_TOKEN\"), add_to_git_credential=True)"],"metadata":{"id":"_Js1D6V2Nv0c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import json\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# ===== Model Initialization =====\n","model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n","\n","# Set padding token\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# ===== Global Variables =====\n","order_session = []\n","conversation_history = []\n","order_confirmed = False\n","\n","# ===== Helper Functions =====\n","\n","# Generate a prompt with dynamic context\n","def generate_prompt(user_input):\n","    formatted_history = \"\\n\".join(conversation_history[-5:])  # Use last 5 entries for context\n","    prompt = (\n","        f\"í˜„ìž¬ ëŒ€í™” ê¸°ë¡:\\n{formatted_history}\\n\"\n","        f\"ê³ ê°ì˜ ìƒˆë¡œìš´ ìž…ë ¥: '{user_input}'\\n\"\n","        \"ì£¼ë¬¸ì„ ì²˜ë¦¬í•˜ê³ , ìˆ˜ì •, ì¶”ê°€ ìš”ì²­ì„ ë°˜ì˜í•˜ì„¸ìš”. \"\n","        \"JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ê³ , ì‚¬ìš©ìžì—ê²Œ í•œêµ­ì–´ë¡œ ìžì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µì„ ìƒì„±í•˜ì„¸ìš”.\"\n","    )\n","    return prompt\n","\n","# Model inference to generate response\n","def model_inference(prompt):\n","    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n","    inputs = inputs.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    with torch.no_grad():\n","        output = model.generate(\n","            inputs.input_ids,\n","            attention_mask=inputs.attention_mask,\n","            max_length=300,\n","            temperature=0.7,\n","            pad_token_id=tokenizer.eos_token_id\n","        )\n","    response = tokenizer.decode(output[0], skip_special_tokens=True)\n","    return response.strip()\n","\n","# Extract JSON data from model output\n","def extract_json_response(model_response):\n","    try:\n","        response_data = model_response.split(\"\\n\")[-1]  # Assume JSON is the last line of response\n","        return json.loads(response_data)\n","    except (json.JSONDecodeError, IndexError):\n","        return None\n","\n","# Update order session based on extracted JSON\n","def update_order_session(json_data):\n","    global order_session\n","    action = json_data.get(\"action\")\n","\n","    if action == \"create_order\" or action == \"add_item\":\n","        order_session.extend(json_data[\"order_items\"])\n","        return \"ìƒˆë¡œìš´ ì£¼ë¬¸ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.\"\n","\n","    elif action == \"modify_order\":\n","        for item in order_session:\n","            if item[\"drink\"] == json_data[\"old_drink\"]:\n","                item.update({\n","                    \"drink\": json_data[\"new_drink\"],\n","                    \"size\": json_data[\"size\"],\n","                    \"temperature\": json_data[\"temperature\"],\n","                    \"quantity\": json_data[\"quantity\"],\n","                    \"add_ons\": json_data.get(\"add_ons\", []),\n","                    \"extra_shots\": json_data.get(\"extra_shots\", 0)\n","                })\n","        return \"ì£¼ë¬¸ì´ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤.\"\n","\n","    elif action == \"cancel_order\":\n","        order_session = []\n","        return \"ì£¼ë¬¸ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.\"\n","\n","    elif action == \"recommend_closest_item\":\n","        return f\"ì£„ì†¡í•©ë‹ˆë‹¤, '{json_data['requested_item']}'ì€(ëŠ”) ë©”ë‰´ì— ì—†ìŠµë‹ˆë‹¤. ëŒ€ì‹  '{json_data['recommended_item']}'ì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.\"\n","\n","    elif action == \"show_order_summary\":\n","        return summarize_order()\n","\n","    elif action == \"complete_order\":\n","        return \"ì£¼ë¬¸ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ê²°ì œëŠ” ì¹´ë“œë¦¬ë”ê¸°ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”. ê°ì‚¬í•©ë‹ˆë‹¤.\"\n","\n","    return \"ì•Œ ìˆ˜ ì—†ëŠ” ìš”ì²­ìž…ë‹ˆë‹¤.\"\n","\n","# Summarize the current order session\n","def summarize_order():\n","    if not order_session:\n","        return \"í˜„ìž¬ ì£¼ë¬¸ ë‚´ì—­ì´ ì—†ìŠµë‹ˆë‹¤.\"\n","\n","    summary = \"ì§€ê¸ˆê¹Œì§€ ì£¼ë¬¸í•˜ì‹  ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\\n\"\n","    for idx, item in enumerate(order_session, start=1):\n","        summary += (\n","            f\"{idx}. {item['drink']} ({item['temperature']}, {item['size']}) \"\n","            f\"{item['quantity']}ìž”\"\n","        )\n","        if item.get(\"add_ons\"):\n","            summary += f\" - ì¶”ê°€ ì˜µì…˜: {', '.join(item['add_ons'])}\"\n","        if item.get(\"extra_shots\"):\n","            summary += f\" - ìƒ· ì¶”ê°€: {item['extra_shots']}ìƒ·\"\n","        summary += \"\\n\"\n","\n","    summary += \"ì¶”ê°€ ì£¼ë¬¸ì´ ìžˆìœ¼ë©´ ë§ì”€í•´ì£¼ì„¸ìš”, ì•„ë‹ˆë©´ ê²°ì œë¥¼ ì§„í–‰í• ê¹Œìš”?\"\n","    return summary\n","\n","# Main function to handle user input\n","def handle_order(user_input):\n","    global conversation_history\n","    conversation_history.append(f\"ì‚¬ìš©ìž: {user_input}\")\n","\n","    prompt = generate_prompt(user_input)\n","    model_response = model_inference(prompt)\n","    json_data = extract_json_response(model_response)\n","\n","    if json_data:\n","        response_message = update_order_session(json_data)\n","        conversation_history.append(f\"í‚¤ì˜¤ìŠ¤í¬: {response_message}\")\n","        return response_message\n","\n","    return \"ì£„ì†¡í•©ë‹ˆë‹¤, ìš”ì²­ì„ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ë§ì”€í•´ì£¼ì„¸ìš”.\"\n","\n","# Main application loop\n","def main():\n","    print(\"ê°€ìƒ ì»¤í”¼ í‚¤ì˜¤ìŠ¤í¬ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!\")\n","    print(\"ì›í•˜ì‹œëŠ” ì£¼ë¬¸ì„ ë§ì”€í•´ì£¼ì„¸ìš”.\")\n","\n","    while not order_confirmed:\n","        user_input = input(\"ê³ ê°: \")\n","        response = handle_order(user_input)\n","        print(\"í‚¤ì˜¤ìŠ¤í¬:\", response)\n","\n","    print(\"ì£¼ë¬¸ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ê²°ì œëŠ” ì¹´ë“œë¦¬ë”ê¸°ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”. ê°ì‚¬í•©ë‹ˆë‹¤!\")\n","\n","# Execute the main application\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"id":"xc6PRRRsGdOz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===== Step 1: Install Required Libraries =====\n","!pip install torch transformers datasets accelerate bitsandbytes\n","!pip install transformers accelerate datasets huggingface_hub"],"metadata":{"id":"1IKEC4TnKiIl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===== Step 1: Install Required Libraries =====\n","!pip install transformers datasets accelerate huggingface_hub --upgrade"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jzmv_KTvX0Ab","executionInfo":{"status":"ok","timestamp":1731146556028,"user_tz":-540,"elapsed":3543,"user":{"displayName":"Nishtha Lath","userId":"10992951628764856112"}},"outputId":"f19b11c6-b4eb-4fcc-9bdf-3e4cefdb2f9c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.1.1)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.26.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.3.1.170)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.4.127)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n"]}]},{"cell_type":"code","source":["# ===== Step 2: Import Libraries =====\n","from transformers import LlamaForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n","from datasets import load_dataset\n","from huggingface_hub import login\n","import pandas as pd\n","import torch\n","\n","# ===== Step 3: Login to Hugging Face =====\n","login(token=\"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\", add_to_git_credential=True)\n","\n","# ===== Step 4: Load and Preprocess Dataset =====\n","data_path = \"/content/dataset.txt\"\n","\n","# Read the dataset\n","with open(data_path, \"r\", encoding=\"utf-8\") as f:\n","    lines = f.readlines()\n","\n","# Convert to a DataFrame\n","data = pd.DataFrame(lines, columns=[\"text\"])\n","\n","# Convert to Hugging Face Dataset\n","dataset = load_dataset(\"text\", data_files=data_path)\n","\n","# Use AutoTokenizer for compatibility\n","model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","# Set padding token if not defined\n","if tokenizer.pad_token is None:\n","    tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n","\n","# Tokenize the dataset\n","def tokenize_data(example):\n","    tokenized = tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n","    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()  # Add labels for computing the loss\n","    return tokenized\n","\n","tokenized_dataset = dataset.map(tokenize_data, batched=True)\n","\n","# Split the dataset into train and evaluation sets\n","train_dataset = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(int(len(tokenized_dataset[\"train\"]) * 0.8)))\n","eval_dataset = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(int(len(tokenized_dataset[\"train\"]) * 0.8), len(tokenized_dataset[\"train\"])))\n","\n","# ===== Step 5: Initialize Model and Training Arguments =====\n","model = LlamaForCausalLM.from_pretrained(model_name)\n","\n","# Resize model embeddings to accommodate new special tokens\n","model.resize_token_embeddings(len(tokenizer))\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"/content/llama-fine-tuned\",\n","    per_device_train_batch_size=4,\n","    per_device_eval_batch_size=4,\n","    learning_rate=1e-5,\n","    num_train_epochs=3,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    logging_dir=\"/content/logs\",\n","    push_to_hub=True,\n","    hub_model_id=\"nishthalath/llama-3b-fine-tuned-kiosk\",\n","    hub_token=\"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\",\n",")\n","\n","# Initialize the Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    tokenizer=tokenizer,\n",")\n","\n","# ===== Step 6: Fine-Tune the Model =====\n","print(\"Starting model fine-tuning...\")\n","trainer.train()\n","\n","# ===== Step 7: Save and Push the Model to Hugging Face Hub =====\n","print(\"Uploading the model to Hugging Face Hub...\")\n","\n","# Push the model and tokenizer to the Hugging Face Hub\n","model.push_to_hub(\"nishthalath/llama-3b-fine-tuned-kiosk\", use_auth_token=\"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\")\n","tokenizer.push_to_hub(\"nishthalath/llama-3b-fine-tuned-kiosk\", use_auth_token=\"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\")\n","\n","print(\"Model uploaded successfully to Hugging Face Hub!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":579,"referenced_widgets":["22b3ed040e2549e9954a03f0fe45a467","e3b7108077a04212b08ea85df0e6366c","6844331022e941e094a4a8a295c9ad5b","0685a0dd67ad4de5a3e2fe7367866cbd","61ad9512d4974a809499e77263b63890","17cf1035d1a345529cc635f60ea0a8cc","3677f4b3010e4cb88a034bfb8f713f0f","217662080c0146fdb2ca05798bfb2b05","3c1e011bcf4845d397f66bdcb032e769","1188114d4f1946fba6cd7550f84d703c","fe4f96a23b1746c6ae917d76213d6901","52743d238871425496b62710ba98b37f","1f9567e2c10e4eb79c8251c3c79c629b","b133a0a636c94bbab0d399d5c131d8d4","483b8f47ee8e4d8ebfd8b0a2d26c6f39","95e98e80422d468ca6c27ca3c0b8afc2","ac3d391b838940bf9bec60cb77439cc7","7a8af183326a4074b3b2c249faec7828","f8dd72e11c0044469686ef0553b5d901","ddd48f0fe09444379787ed7bd82ec795","957bfbdcdec04748adb93cb60d1f2c0f","51de19d9f6774d0aa41e65961f3fb7a2"]},"id":"QCRLYCJsIB8t","executionInfo":{"status":"error","timestamp":1731147008577,"user_tz":-540,"elapsed":24102,"user":{"displayName":"Nishtha Lath","userId":"10992951628764856112"}},"outputId":"a2592f7f-4521-4fb8-a128-79da006c127b"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/319 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22b3ed040e2549e9954a03f0fe45a467"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52743d238871425496b62710ba98b37f"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","<ipython-input-8-931fa3720111>:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"stream","name":"stdout","text":["Starting model fine-tuning...\n"]},{"output_type":"error","ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 42.81 MiB is free. Process 146371 has 39.51 GiB memory in use. Of the allocated memory 38.57 GiB is allocated by PyTorch, and 456.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-931fa3720111>\u001b[0m in \u001b[0;36m<cell line: 76>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# ===== Step 6: Fine-Tune the Model =====\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting model fine-tuning...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;31m# ===== Step 7: Save and Push the Model to Hugging Face Hub =====\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2112\u001b[0m                 \u001b[0;31m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2113\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2114\u001b[0;31m                 return inner_training_loop(\n\u001b[0m\u001b[1;32m   2115\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m                     \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2479\u001b[0m                     )\n\u001b[1;32m   2480\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2481\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2483\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3610\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3611\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3612\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3613\u001b[0m             \u001b[0;31m# Finally we need to normalize the loss for reporting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3614\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2240\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2241\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 39.56 GiB of which 42.81 MiB is free. Process 146371 has 39.51 GiB memory in use. Of the allocated memory 38.57 GiB is allocated by PyTorch, and 456.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"]}]}]}