{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPTgWxjoeSp+u0Cy9MMOrxE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"eiaTyDDmPDY5"},"outputs":[],"source":["from transformers import pipeline\n","import json\n","\n","# Initialize the Korean model for text generation\n","model = pipeline('text-generation', model='Bllossom/llama-3.2-Korean-Bllossom-3B', device=0)\n","\n","# Initialize conversation history, summary history, and order confirmation flag\n","conversation_history = []  # Stores each customer input as a string entry in the format: \"Customer's X Input: [input]\"\n","summary_history = []       # Stores each cumulative summary of orders\n","order_confirmed = False    # Tracks if the order is finalized\n","\n","# Function to generate prompt based on conversation history and new input\n","def generate_prompt(conversation_history, user_input):\n","    # Format the conversation history and cumulative summary as a single string\n","    formatted_history = \"\\n\".join(conversation_history) if conversation_history else \"none\"\n","    formatted_summary = \"\\n\".join(summary_history) if summary_history else \"none\"\n","\n","    base_prompt = f\"\"\"\n","    You are operating a virtual coffee kiosk that receives speech-to-text (STT) inputs from customers placing coffee orders. Your role is to understand and process these inputs, respond naturally in Korean, and generate a structured JSON file with the correct details for backend processing.\n","\n","    **Key Requirements**:\n","    - **Menu Items**: The kiosk offers the following drinks:\n","    - Hot Drinks: 허브티 (always served hot)\n","    - Iced Only Drinks: 토마토주스, 키위주스, 망고스무디, 딸기스무디, 레몬에이드, 복숭아아이스티 (always served iced)\n","    - Hot and Iced Coffee: 아메리카노, 라떼, 카푸치노, 카페모카, 바닐라라떼, 에스프레소, 카라멜마끼아또\n","    - Specialty Drinks: 초콜릿라떼 (available in both hot and iced versions)\n","\n","    - **Default Values**:\n","        - Use default size \"미디움\" and temperature \"핫\" only if the customer does not specify these details.\n","    - **Do Not Make Assumptions**:\n","        - If the customer specifies temperature or size, do not override it with defaults. For instance, if they say \"아이스 라떼 두잔 주세요\", the output should indicate \"아이스\" without changing it to \"핫\".\n","    - **Current Conversation History** is a single-line cumulative log of all customer requests so far in this session. starting from 1\n","    **Customer Input and Expected Output Format**:\n","    - Each response should have:\n","      1. **Natural Language Confirmation**: Respond in Korean, starting with an action confirmation such as \"[Drink] [quantity] 주문되었습니다.\" and follow with a full summary of all items ordered so far, beginning with \"지금까지 주문하신 내용은 다음과 같습니다:\".(also if there are any instance in history it should be added after this sentence)\n","      2. **Structured JSON Output**: Each JSON output should only contain the items directly requested in the latest input, not a full history.\n","\n","    **JSON Output Format**:\n","    - The JSON should be structured as follows:\n","      ```json\n","      {{\n","          \"action\": \"[action_type]\",\n","          \"order_items\": [\n","              {{\n","                  \"drink\": \"[Drink Name]\",\n","                  \"size\": \"[Size]\",\n","                  \"temperature\": \"[Temperature]\",\n","                  \"quantity\": [Quantity],\n","                  \"add_ons\": [List of add-ons if any],\n","                  \"extra_shots\": [Number of extra shots if any]\n","              }}\n","          ]\n","      }}\n","      ```\n","      - **Example JSON Output**:\n","        ```json\n","        {{\n","            \"action\": \"create_order\",\n","            \"order_items\": [\n","                {{\n","                    \"drink\": \"아메리카노\",\n","                    \"size\": \"미디움\",\n","                    \"temperature\": \"핫\",\n","                    \"quantity\": 1,\n","                    \"add_ons\": [],\n","                    \"extra_shots\": 0\n","                }}\n","            ]\n","        }}\n","        ```\n","\n","    **Available Actions for JSON Output**:\n","    - **create_order**: For new drink orders.\n","    - **add_item**: For adding a new item to the current order.\n","    - **modify_order**: For changing an existing item (e.g., modifying size or temperature).\n","    - **cancel_order**: To remove an order item or reset the order.\n","    - **recommend_closest_item**: If a requested item is unavailable, recommend the closest item.\n","    - **show_order_summary**: Display a summary of all items ordered so far.\n","    - **complete_order**: Finalize the order after confirmation.\n","\n","    **Specific Scenarios and Expected Outputs**:\n","    - **Creating a New Order**:\n","      - **Customer Input**: \"아메리카노 4잔 주세요.\"\n","      - **Natural Language Response**: \"아메리카노 4잔 주문되었습니다. 지금까지 주문하신 내용은 다음과 같습니다:\n","      -아메리카노 4잔 (핫, 미디움)\"\n","      - **JSON Output**:\n","        ```json\n","        {{\n","          \"action\": \"create_order\",\n","          \"order_items\": [\n","            {{\n","              \"drink\": \"아메리카노\",\n","              \"size\": \"미디움\",\n","              \"temperature\": \"핫\",\n","              \"quantity\": 4,\n","              \"add_ons\": [],\n","              \"extra_shots\": 0\n","            }}\n","          ]\n","        }}\n","        ```\n","\n","    - **Requesting Order Summary**:\n","      - **Customer Input**: \"내가 지금까지 뭘 주문했지?\"\n","      - **Natural Language Response**: \"지금까지 주문하신 내용은 다음과 같습니다:\n","      -아메리카노 4잔(핫, 미디움)\n","      -카페라떼 라지 2잔 (핫, 라지)\"\n","      - **JSON Output**: None (as it is just a summary request without any new action).\n","\n","    - **Modifying an Existing Order**:\n","      - **Customer Input**: \"주문한거 아이스 라떼로 바꿔줘.\"\n","      - **Natural Language Response**: \"주문이 아메리카노에서 아이스 라떼로 변경되었습니다. 주문하신 내용은 다음과 같습니다:\n","      -라떼 1잔 (아이스, 미디움)\"\n","      - **JSON Output**:\n","        ```json\n","        {{\n","          \"action\": \"modify_order\",\n","          \"old_drink\": \"아메리카노\",\n","          \"new_drink\": \"라떼\",\n","          \"size\": \"미디움\",\n","          \"temperature\": \"아이스\",\n","          \"quantity\": 1,\n","          \"add_ons\": [],\n","          \"extra_shots\": 0\n","        }}\n","        ```\n","\n","    - **Short Names or Misspellings**:\n","      - Recognize common shorthand or misspellings. For example:\n","        - \"아아\" should be interpreted as \"아이스 아메리카노\".\n","        - \"뜨아\" should be interpreted as \"핫 아메리카노\".\n","\n","    - **Unavailable Items**:\n","      - If the customer requests an item not on the menu, respond politely and recommend a similar item if available.\n","      - **Example**:\n","        - **Customer Input**: \"초코라떼 주세요.\"\n","        - **Natural Language Response**: \"죄송합니다, 초코라떼는 메뉴에 없습니다. 대신 초콜릿라떼를 추천드립니다.\"\n","        - **JSON Output**:\n","          ```json\n","          {{\n","            \"action\": \"recommend_closest_item\",\n","            \"requested_item\": \"초코라떼\",\n","            \"recommended_item\": \"초콜릿라떼\"\n","          }}\n","          ```\n","\n","    - **Order Confirmation**:\n","      - **Customer Input**: \"주문 완료할게요.\"\n","      - **Natural Language Response**: \"주문이 완료되었습니다. 결제는 카드리더기를 사용해주세요. 감사합니다.\"\n","      - **JSON Output**: Clear/reset for the next session.\n","\n","    **Response Rules**:\n","    - Treat each new input as part of the same order until \"주문 완료할게\" is received, which finalizes the order.\n","    - Always confirm the latest action first in the natural language response, followed by a full order summary.\n","    - Ensure each JSON output reflects only the latest request, not the entire order history.\n","\n","    Based on this information,\n","    (keep in mind that) Each JSON output should only contain the items directly requested in **Customer's New Input**, not a full history.\n","    if **Current Conversation History**:\n","    {formatted_history}\n","\n","    based on **Cumulative Order Summary So Far**:\n","    {formatted_summary}\n","\n","    and **Customer's New Input**: \"{user_input}\"\n","\n","    generate the appropriate natural language response and JSON output\n","    \"\"\"\n","    return base_prompt.strip()\n","\n","# Function to get response from the Korean model pipeline\n","def get_response(prompt):\n","    response = model(prompt, max_new_tokens=150, num_return_sequences=1, temperature=0.1, top_p=0.9, truncation=True)\n","    return response[0]['generated_text']\n","\n","# Function to parse response into natural language and JSON output\n","def parse_response(response):\n","    try:\n","        natural_language_response, json_output = response.split(\"JSON Output:\")\n","        json_data = json.loads(json_output.strip())\n","        return natural_language_response.strip(), json_data\n","    except ValueError as e:\n","        print(\"Error parsing response:\", e)\n","        return response, None\n","\n","# Main interaction loop\n","print(\"Welcome to the virtual coffee kiosk! What would you like to order?\")\n","input_counter = 1\n","\n","while not order_confirmed:\n","    # Take user input\n","    user_input = input(\"Customer: \")\n","\n","    # Check if customer confirms the order\n","    if \"주문 완료할게\" in user_input:\n","        order_confirmed = True\n","        print(\"Kiosk: 주문이 완료되었습니다. 결제는 카드리더기를 사용해주세요. 감사합니다.\")\n","        conversation_history.clear()\n","        summary_history.clear()\n","        continue\n","\n","\n","    # Generate the prompt based on the conversation history and new user input\n","    prompt = generate_prompt(conversation_history, user_input)\n","\n","    # Get the model's response\n","    response = get_response(prompt)\n","\n","    # Add the latest user input to the conversation history\n","    conversation_history.append(f\"Customer's {input_counter} Input: {user_input}\")\n","    input_counter += 1\n","\n","    # Parse the model's response\n","    natural_language_response, json_output = parse_response(response)\n","\n","    # Save natural language order summary to `summary_history`\n","    if natural_language_response:\n","        summary_line = natural_language_response.split(\"\\n\", 1)[1]  # Extracts summary part\n","        summary_history.append(summary_line)\n","\n","    # Print the final natural language response for the customer\n","    #print(\"Kiosk:\", natural_language_response)\n","    # Print only the Natural Language Response and JSON Output in the required format\n","    print(\"**Natural Language Response**:\")\n","    print(natural_language_response)\n","    print(\"\\n**JSON Output**:\")\n","    print(json.dumps(json_output, indent=2, ensure_ascii=False))  # Pretty-print JSON output\n","\n","    # Special handling for order summary requests\n","    if \"내가 지금까지 뭘 주문했지\" in user_input:\n","        # Provide cumulative order summary as a natural response without JSON output\n","        print(\"Kiosk:\", \"지금까지의 주문내역입니다:\\n\" + \"\\n\".join(summary_history))\n","\n","    # Handle order cancellation requests\n","    if \"주문 취소\" in user_input:\n","        # Clear conversation history and summary history for a reset\n","        print(\"Kiosk: 주문이 취소되었습니다. 새 주문을 시작해 주세요.\")\n","        conversation_history.clear()\n","        summary_history.clear()\n","        input_counter = 1\n","\n","# End of the session\n","print(\"Thank you for using the coffee kiosk!\")\n"]},{"cell_type":"code","source":["!apt-get install git\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cwLEaWeygLRM","executionInfo":{"status":"ok","timestamp":1730729289489,"user_tz":-540,"elapsed":11191,"user":{"displayName":"Coffee Kiosk","userId":"02539563182731550933"}},"outputId":"db490500-ca6b-432b-928e-e9ea7bae03b4"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","git is already the newest version (1:2.34.1-1ubuntu1.11).\n","0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"]}]},{"cell_type":"code","source":["!git config --global user.name \"fatih0122\"\n","!git config --global user.email \"m.fatih012001@gmail.com\""],"metadata":{"id":"sN8DsYpogNz9","executionInfo":{"status":"ok","timestamp":1730729538021,"user_tz":-540,"elapsed":498,"user":{"displayName":"Coffee Kiosk","userId":"02539563182731550933"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/AI-coffee-Kiosk/Prompting-for-llama.git\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EKP4h02-hPkq","executionInfo":{"status":"ok","timestamp":1730729971650,"user_tz":-540,"elapsed":617,"user":{"displayName":"Coffee Kiosk","userId":"02539563182731550933"}},"outputId":"6389e5db-f01b-46e0-9f7c-f3fedd23c4bf"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Prompting-for-llama'...\n","warning: You appear to have cloned an empty repository.\n"]}]},{"cell_type":"code","source":["\n","!cp \"/content/promptforkrmodel.ipynb\" \"/content/Prompting-for-llama/\"\n"],"metadata":{"id":"nNgTQATLi7dS","executionInfo":{"status":"ok","timestamp":1730730472575,"user_tz":-540,"elapsed":326,"user":{"displayName":"Coffee Kiosk","userId":"02539563182731550933"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["%cd /content/Prompting-for-llama/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NmNZPTYgk4yF","executionInfo":{"status":"ok","timestamp":1730730529913,"user_tz":-540,"elapsed":343,"user":{"displayName":"Coffee Kiosk","userId":"02539563182731550933"}},"outputId":"f2b20d97-23d4-46ec-89de-bd0c527dba49"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/Prompting-for-llama\n"]}]},{"cell_type":"code","source":["!git remote set-url origin https://github.com/AI-coffee-Kiosk/Prompting-for-llama.git\n"],"metadata":{"id":"_B0Yi_0Ax8WF","executionInfo":{"status":"ok","timestamp":1730733921290,"user_tz":-540,"elapsed":402,"user":{"displayName":"Coffee Kiosk","userId":"02539563182731550933"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["!git add promptforkrmodel.ipynb\n","!git commit -m \"Prompting llama for coffee orders\"\n","!git push origin main\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"go2oFg6FlJYi","executionInfo":{"status":"ok","timestamp":1730731258626,"user_tz":-540,"elapsed":734,"user":{"displayName":"Coffee Kiosk","userId":"02539563182731550933"}},"outputId":"9cad75ed-a436-4e09-c686-81a76133cab6"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["On branch main\n","Your branch is based on 'origin/main', but the upstream is gone.\n","  (use \"git branch --unset-upstream\" to fixup)\n","\n","nothing to commit, working tree clean\n","error: src refspec master does not match any\n","\u001b[31merror: failed to push some refs to 'https://github.com/AI-coffee-Kiosk/Prompting-for-llama.git'\n","\u001b[m"]}]},{"cell_type":"code","source":["from google.colab import files\n","files.upload()  # upload id_rsa\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":342},"id":"sdX9j5bVosXZ","executionInfo":{"status":"error","timestamp":1730731504490,"user_tz":-540,"elapsed":8082,"user":{"displayName":"Coffee Kiosk","userId":"02539563182731550933"}},"outputId":"09ab53a6-75db-4dcb-b3f3-68f5f6eb71db"},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-be27a2e4-5724-45d0-a698-2d05e8c2dee0\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-be27a2e4-5724-45d0-a698-2d05e8c2dee0\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-a379db544ed2>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# upload id_rsa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    165\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    166\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["!mkdir -p ~/.ssh\n","!mv id_rsa ~/.ssh/id_rsa\n","!chmod 600 ~/.ssh/id_rsa\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pSr6e9UlovvH","executionInfo":{"status":"ok","timestamp":1730731534062,"user_tz":-540,"elapsed":716,"user":{"displayName":"Coffee Kiosk","userId":"02539563182731550933"}},"outputId":"f3f741f6-e0ba-40ae-8408-8bce8aee85fd"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["mv: cannot stat 'id_rsa': No such file or directory\n","chmod: cannot access '/root/.ssh/id_rsa': No such file or directory\n"]}]},{"cell_type":"code","source":["!ssh-keygen -t rsa -b 4096 -C \"m.fatih012001@gmail.com\" -f ~/.ssh/id_rsa -N \"\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IoLjlavcsQ5e","executionInfo":{"status":"ok","timestamp":1730732465598,"user_tz":-540,"elapsed":3150,"user":{"displayName":"Coffee Kiosk","userId":"02539563182731550933"}},"outputId":"06660e61-118e-470c-ac0e-57fbee85104f"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Generating public/private rsa key pair.\n","Your identification has been saved in /root/.ssh/id_rsa\n","Your public key has been saved in /root/.ssh/id_rsa.pub\n","The key fingerprint is:\n","SHA256:keLW90aiQbNgOjBtRDkKg4VpZ7c3hg++7DApPhOaN40 m.fatih012001@gmail.com\n","The key's randomart image is:\n","+---[RSA 4096]----+\n","|.+..o.           |\n","|*. =o.   .       |\n","|.o=.+.* =        |\n","|  .+ B O +       |\n","|    + B S o .    |\n","| .  .+ . + +     |\n","|.o.*. . .   o    |\n","|+oE +o     .     |\n","| oo...           |\n","+----[SHA256]-----+\n"]}]},{"cell_type":"code","source":["!cat ~/.ssh/id_rsa.pub\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U2z-11sksdQQ","executionInfo":{"status":"ok","timestamp":1730732495491,"user_tz":-540,"elapsed":298,"user":{"displayName":"Coffee Kiosk","userId":"02539563182731550933"}},"outputId":"6bf81e0f-671f-43b8-9743-4fff74a21ef0"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQCsjzWMjYmRpGgnMbUaEn/A/Bylm2gjXV+CQQ48d1WEGbzuWGC7OfxaQXxvMvnTu91YI+AnTQNjiKhF2XQEP6NYtGVYzUEz+qFXQNNcR7MkOsWNa3D4AlQ3o29YjxlvpBckgrVbedWgJQTzeh1wgutglTjxdMxUR+yprc7LLK39/JIu1ehNqh1B18OO9j3isvbHEETZbIB9vZ+O+H1tWT01mF4Ve3VJKUgdNi6EPSYERU85YwgZaApn8tY1Hmz7hUTVpoezoCE85U7VouVR3KW2J9wSnVD8a3AptytNplE66/gibDaUPdEk0SOd2wM+f4psDoVAlQGzIiobI3r4CB+ymBjD7xTKgxPxWF16NR1IwNwMBJGX6QZEZaIrQPer791wETamHBSYNB5MVWOttpJtKE2cdvYR6iojfAWgFZJx2eBvbeorfkvbEGLlLC3nn6uV9JDvlttCxox193+/F80SnXVHOS/ljs38THi7UtI4Yg0ir0kYV2dm9CA2ZT0XVYGVsDvXbRpvU/LRkRClqC1PNuxCAJ95v7aWSfW+sWcy3WFn3/M+JelUfq6d9M4qP2oTDh7iw/ZRNsdYXqEiR6OcF9lQ/nXhvVdiAmwPmjm8s/9ZDhffiUkSgRPdW7c98ebooEUL48KTzzB+BYKdYDpYu6aQ1CmxwX77U19e1MwNTQ== m.fatih012001@gmail.com\n"]}]},{"cell_type":"code","source":["!git remote set-url origin git@github.com:AI-coffee-Kiosk/Prompting-for-llama.git\n"],"metadata":{"id":"O7WtdBtktQU4","executionInfo":{"status":"ok","timestamp":1730732732902,"user_tz":-540,"elapsed":931,"user":{"displayName":"Coffee Kiosk","userId":"02539563182731550933"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["!mkdir -p ~/.ssh\n","!ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_nbToD26trnH","executionInfo":{"status":"ok","timestamp":1730732806048,"user_tz":-540,"elapsed":284,"user":{"displayName":"Coffee Kiosk","userId":"02539563182731550933"}},"outputId":"b41af510-2842-418f-aa36-08c526a8e8e8"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["# github.com:22 SSH-2.0-babeld-d5e089cf5\n"]}]},{"cell_type":"code","source":["!ssh -T git@github.com"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FpKAUDJ6tfLQ","executionInfo":{"status":"ok","timestamp":1730732809949,"user_tz":-540,"elapsed":615,"user":{"displayName":"Coffee Kiosk","userId":"02539563182731550933"}},"outputId":"7494f41e-c19d-4809-a5a7-6b9bd37e5951"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Hi fatih0122! You've successfully authenticated, but GitHub does not provide shell access.\n"]}]},{"cell_type":"code","source":["!git add .\n","!git commit -m \"Llama prompting for cofee kiosk\"\n","!git push origin main\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8C6CDl8Rt2SD","executionInfo":{"status":"ok","timestamp":1730732902152,"user_tz":-540,"elapsed":1326,"user":{"displayName":"Coffee Kiosk","userId":"02539563182731550933"}},"outputId":"ba706f1c-07e1-4bcb-f5ed-44557a3bfeb3"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["On branch main\n","Your branch is based on 'origin/main', but the upstream is gone.\n","  (use \"git branch --unset-upstream\" to fixup)\n","\n","nothing to commit, working tree clean\n","Enumerating objects: 3, done.\n","Counting objects: 100% (3/3), done.\n","Delta compression using up to 2 threads\n","Compressing objects: 100% (3/3), done.\n","Writing objects: 100% (3/3), 4.81 KiB | 4.81 MiB/s, done.\n","Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\n","To github.com:AI-coffee-Kiosk/Prompting-for-llama.git\n"," * [new branch]      main -> main\n"]}]}]}